"""
AI Orchestrator Service
Manages AI models, agents, and context for complex enterprise data analysis
"""

import logging
from typing import Dict, Any, List, Optional
from datetime import datetime, timezone
import json
import re

from .litellm_service import LiteLLMService

logger = logging.getLogger(__name__)


class AIOrchestrator:
    """Orchestrates AI models and agents for complex data analysis"""

    def __init__(self):
        self.litellm_service = LiteLLMService()
        self.analysis_cache = {}

    async def orchestrate_analysis(
        self,
        query: str,
        data_context: Dict[str, Any],
        user_context: Dict[str, Any] = None,
    ) -> Dict[str, Any]:
        """Orchestrate comprehensive AI analysis with multiple specialized agents"""
        try:
            logger.info(f"üöÄ Starting AI orchestration for query: {query[:100]}...")

            # Phase 1: Data Understanding Agent
            data_understanding = await self._data_understanding_agent(
                query, data_context
            )

            # Phase 2: Analysis Planning Agent
            analysis_plan = await self._analysis_planning_agent(
                query, data_context, data_understanding
            )

            # Phase 3: SQL Generation Agent (if applicable)
            sql_queries = []
            if data_context.get("type") in ["database", "warehouse", "cube"]:
                sql_queries = await self._sql_generation_agent(
                    query, data_context, analysis_plan
                )

            # Phase 4: Chart Recommendation Agent
            chart_recommendations = await self._chart_recommendation_agent(
                query, data_context, analysis_plan
            )

            # Phase 5: Insight Generation Agent
            insights = await self._insight_generation_agent(
                query, data_context, analysis_plan, sql_queries
            )

            # Phase 6: Business Context Agent
            business_context = await self._business_context_agent(
                query, data_context, insights
            )

            # Phase 7: Final Synthesis
            final_response = await self._synthesize_response(
                query,
                data_context,
                analysis_plan,
                sql_queries,
                chart_recommendations,
                insights,
                business_context,
            )

            return {
                "success": True,
                "content": final_response,
                "analysis_components": {
                    "data_understanding": data_understanding,
                    "analysis_plan": analysis_plan,
                    "sql_queries": sql_queries,
                    "chart_recommendations": chart_recommendations,
                    "insights": insights,
                    "business_context": business_context,
                },
                "execution_metadata": {
                    "timestamp": datetime.now().isoformat(),
                    "data_source": data_context.get("type", "unknown"),
                    "total_rows": data_context.get("total_rows", 0),
                    "ai_model": "orchestrated-multi-agent",
                    "phases_completed": 7,
                },
            }

        except Exception as e:
            logger.error(f"‚ùå AI orchestration failed: {str(e)}")
            return await self._fallback_analysis(query, data_context, str(e))

    async def analyze_data_with_context(
        self,
        user_query: str,
        data_sources: List[Dict],
        conversation_history: List[Dict] = None,
        analysis_type: str = "general",
        user_context: Dict = None,
    ) -> Dict:
        """
        Enhanced data analysis with full context awareness of connected data sources
        Prioritizes Cube.js when available, falls back to database, then files
        """
        try:
            logger.info(
                f"üöÄ Starting AI Orchestrator analysis for query: {user_query[:100]}..."
            )
            logger.info(f"üìä Data sources count: {len(data_sources)}")
            logger.info(f"üìä Data sources received: {data_sources}")

            # Build comprehensive context from data sources
            logger.info("üîç Building data context...")
            context = await self._build_data_context(data_sources)
            logger.info(
                f"üìã Context built successfully: {len(context.get('cube_js_sources', []))} cube sources, {len(context.get('database_sources', []))} database sources, {len(context.get('file_sources', []))} file sources"
            )
            logger.info(f"üìã Full context: {context}")

            # Determine best analysis approach based on available sources
            logger.info("üéØ Determining analysis strategy...")
            analysis_strategy = self._determine_analysis_strategy(
                data_sources, user_query, user_context
            )
            logger.info(f"üéØ Analysis strategy: {analysis_strategy}")

            # Build enhanced prompt with data context
            logger.info("üìù Building enhanced prompt...")
            enhanced_prompt = self._build_enhanced_prompt(
                user_query,
                context,
                analysis_strategy,
                conversation_history,
                user_context,
            )
            logger.info(f"üìù Enhanced prompt length: {len(enhanced_prompt)} characters")

            # Execute analysis based on strategy
            logger.info(f"‚ö° Executing {analysis_strategy} analysis...")
            if analysis_strategy == "cube_js":
                result = await self._execute_cube_analysis(
                    user_query, context, enhanced_prompt
                )
            elif analysis_strategy == "database":
                result = await self._execute_database_analysis(
                    user_query, context, enhanced_prompt
                )
            else:
                result = await self._execute_file_analysis(
                    user_query, context, enhanced_prompt
                )

            logger.info(
                f"‚úÖ Analysis execution completed: {result.get('type', 'unknown')}"
            )

            # Validate and enhance results
            logger.info("üîç Validating analysis result...")
            validated_result = await self._validate_analysis_result(result, context)

            return {
                "success": True,
                "analysis": validated_result,
                "strategy_used": analysis_strategy,
                "data_sources_used": [ds["id"] for ds in data_sources],
                "context_summary": context.get("summary", ""),
                "execution_metadata": {
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "model_used": self._get_current_model(),
                    "analysis_type": analysis_type,
                },
            }

        except Exception as e:
            logger.error(f"‚ùå Data analysis failed: {str(e)}")
            logger.error(f"‚ùå Exception type: {type(e).__name__}")
            import traceback

            logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
            return {
                "success": False,
                "error": f"Analysis failed: {str(e)}",
                "fallback_suggestion": "Try rephrasing your query or check data source connections",
            }

    async def _build_data_context(self, data_sources: List[Dict]) -> Dict:
        """Build comprehensive context from all connected data sources"""
        context = {
            "summary": "",
            "cube_js_sources": [],
            "database_sources": [],
            "file_sources": [],
            "available_metrics": [],
            "available_dimensions": [],
            "table_schemas": {},
            "relationships": [],
            "data_source_inventory": [],
            "analysis_capabilities": {},
        }

        logger.info(f"üîç Building data context for {len(data_sources)} data sources")
        logger.info(f"üîç Data sources received: {data_sources}")

        for source in data_sources:
            source_id = source.get("id", "unknown")
            source_type = source.get("type", "unknown")
            source_name = source.get("name", "unknown")

            logger.info(
                f"üìä Processing {source_type} source: {source_name} ({source_id})"
            )

            # Add to inventory
            context["data_source_inventory"].append(
                {
                    "id": source_id,
                    "name": source_name,
                    "type": source_type,
                    "status": "active",
                    "capabilities": self._get_source_capabilities(source),
                }
            )

            if source_type == "cube":
                cube_context = await self._get_cube_context(source)
                context["cube_js_sources"].append(cube_context)
                context["available_metrics"].extend(cube_context.get("measures", []))
                context["available_dimensions"].extend(
                    cube_context.get("dimensions", [])
                )

            elif source_type == "database":
                db_context = await self._get_database_context(source)
                context["database_sources"].append(db_context)
                context["table_schemas"].update(db_context.get("tables", {}))

            elif source_type == "file":
                logger.info(f"üìÅ Processing file source: {source_name}")
                file_context = await self._get_file_context(source)
                context["file_sources"].append(file_context)
                logger.info(
                    f"üìÅ Added file source to context: {len(context['file_sources'])} total"
                )
            else:
                logger.warning(
                    f"‚ö†Ô∏è Unknown source type: {source_type} for source {source_name}"
                )

        # Build comprehensive summary and capabilities
        context["summary"] = self._build_context_summary(context)
        context["analysis_capabilities"] = self._build_analysis_capabilities(context)

        logger.info(
            f"‚úÖ Data context built: {len(context['cube_js_sources'])} cube, {len(context['database_sources'])} database, {len(context['file_sources'])} file sources"
        )
        logger.info(f"‚úÖ Final context keys: {list(context.keys())}")
        return context

    def _get_source_capabilities(self, source: Dict) -> Dict:
        """Get analysis capabilities for a data source"""
        source_type = source.get("type", "unknown")
        capabilities = {
            "can_aggregate": False,
            "can_dimension": False,
            "can_filter": False,
            "can_time_series": False,
            "can_join": False,
            "data_types": [],
            "supported_operations": [],
        }

        if source_type == "cube":
            capabilities.update(
                {
                    "can_aggregate": True,
                    "can_dimension": True,
                    "can_filter": True,
                    "can_time_series": True,
                    "can_join": True,
                    "supported_operations": [
                        "semantic_queries",
                        "pre_aggregations",
                        "time_granularity",
                        "business_logic",
                    ],
                }
            )
        elif source_type == "database":
            capabilities.update(
                {
                    "can_aggregate": True,
                    "can_dimension": True,
                    "can_filter": True,
                    "can_time_series": True,
                    "can_join": True,
                    "supported_operations": [
                        "sql_queries",
                        "complex_joins",
                        "stored_procedures",
                        "views",
                    ],
                }
            )
        elif source_type == "file":
            capabilities.update(
                {
                    "can_aggregate": True,
                    "can_dimension": True,
                    "can_filter": True,
                    "can_time_series": False,  # Depends on data structure
                    "can_join": False,
                    "supported_operations": [
                        "column_analysis",
                        "statistical_summary",
                        "pattern_recognition",
                    ],
                }
            )

        return capabilities

    def _build_analysis_capabilities(self, context: Dict) -> Dict:
        """Build comprehensive analysis capabilities summary"""
        capabilities = {
            "total_sources": len(context["data_source_inventory"]),
            "cube_js_available": len(context["cube_js_sources"]) > 0,
            "database_available": len(context["database_sources"]) > 0,
            "file_available": len(context["file_sources"]) > 0,
            "recommended_strategy": self._determine_recommended_strategy(context),
            "available_metrics_count": len(context["available_metrics"]),
            "available_dimensions_count": len(context["available_dimensions"]),
            "supported_chart_types": self._get_supported_chart_types(context),
            "query_capabilities": self._get_query_capabilities(context),
        }

        return capabilities

    def _determine_analysis_strategy(
        self,
        data_sources: List[Dict],
        user_query: str = "",
        user_selection: Dict = None,
    ) -> str:
        """Determine the best analysis strategy based on available sources and user intent"""
        # User selection takes highest priority
        if user_selection and user_selection.get("selected_data_source_id"):
            selected_id = user_selection["selected_data_source_id"]
            selected_source = next(
                (ds for ds in data_sources if ds["id"] == selected_id), None
            )
            if selected_source:
                logger.info(
                    f"üéØ User explicitly selected {selected_source['type']} source: {selected_source['name']}"
                )
                return f"{selected_source['type']}_user_selected"

        # Analyze user intent from query
        query_intent = self._analyze_user_intent(user_query)
        logger.info(f"üß† User intent detected: {query_intent}")

        # Check for active data sources from data panel state
        active_sources = self._get_active_data_sources(data_sources, user_selection)
        logger.info(f"üìä Active data sources: {[s['type'] for s in active_sources]}")

        # Prioritize based on user intent and active sources
        if active_sources:
            # User has active sources - prioritize based on intent
            if query_intent.get("prefers_cube") and any(
                s["type"] == "cube" for s in active_sources
            ):
                return "cube_js_intent_driven"
            elif query_intent.get("prefers_database") and any(
                s["type"] == "database" for s in active_sources
            ):
                return "database_intent_driven"
            elif query_intent.get("prefers_file") and any(
                s["type"] == "file" for s in active_sources
            ):
                return "file_intent_driven"
            else:
                # Use the first active source type
                return f"{active_sources[0]['type']}_active_source"

        # Fallback to general availability
        has_cube = any(ds["type"] == "cube" for ds in data_sources)
        has_database = any(ds["type"] == "database" for ds in data_sources)
        has_files = any(ds["type"] == "file" for ds in data_sources)

        if has_cube:
            return "cube_js_available"  # Prioritize Cube.js for semantic analysis
        elif has_database:
            return "database_available"  # Use database for SQL analysis
        elif has_files:
            return "file_available"  # Fall back to file analysis
        else:
            return "general_guidance"  # No data sources available

    def _analyze_user_intent(self, user_query: str) -> Dict:
        """Analyze user query to determine analysis intent and preferences"""
        if not user_query:
            return {}

        query_lower = user_query.lower()
        intent = {
            "prefers_cube": False,
            "prefers_database": False,
            "prefers_file": False,
            "analysis_type": "general",
            "complexity_level": "basic",
            "time_sensitive": False,
            "business_focused": False,
        }

        # Check for Cube.js specific language
        cube_keywords = [
            "semantic",
            "business logic",
            "pre-aggregated",
            "measures",
            "dimensions",
            "cube",
            "analytics",
            "business intelligence",
            "kpi",
            "dashboard",
        ]
        if any(keyword in query_lower for keyword in cube_keywords):
            intent["prefers_cube"] = True
            intent["business_focused"] = True

        # Check for database specific language
        database_keywords = [
            "sql",
            "query",
            "join",
            "table",
            "schema",
            "database",
            "warehouse",
            "complex",
            "advanced",
            "technical",
            "raw data",
            "custom logic",
        ]
        if any(keyword in query_lower for keyword in database_keywords):
            intent["prefers_database"] = True
            intent["complexity_level"] = "advanced"

        # Check for file specific language
        file_keywords = [
            "upload",
            "file",
            "csv",
            "excel",
            "spreadsheet",
            "simple",
            "quick",
            "basic",
            "overview",
            "summary",
        ]
        if any(keyword in query_lower for keyword in file_keywords):
            intent["prefers_file"] = True
            intent["complexity_level"] = "basic"

        # Determine analysis type
        if any(
            word in query_lower
            for word in ["trend", "time", "over time", "growth", "forecast"]
        ):
            intent["analysis_type"] = "time_series"
            intent["time_sensitive"] = True
        elif any(
            word in query_lower
            for word in ["compare", "vs", "versus", "difference", "ranking"]
        ):
            intent["analysis_type"] = "comparative"
        elif any(
            word in query_lower
            for word in ["correlation", "relationship", "pattern", "insight"]
        ):
            intent["analysis_type"] = "exploratory"
        elif any(
            word in query_lower for word in ["summary", "overview", "total", "count"]
        ):
            intent["analysis_type"] = "summary"

        return intent

    def _get_active_data_sources(
        self, data_sources: List[Dict], user_selection: Dict = None
    ) -> List[Dict]:
        """Get currently active data sources based on user selection and data panel state"""
        active_sources = []

        # Check for explicit user selection
        if user_selection:
            selected_id = user_selection.get("selected_data_source_id")
            if selected_id:
                selected_source = next(
                    (ds for ds in data_sources if ds["id"] == selected_id), None
                )
                if selected_source:
                    active_sources.append(selected_source)
                    logger.info(
                        f"üéØ User selected active source: {selected_source['name']} ({selected_source['type']})"
                    )

        # Check for multiple active sources (data panel state)
        if user_selection and user_selection.get("active_data_sources"):
            active_ids = user_selection["active_data_sources"]
            for active_id in active_ids:
                source = next(
                    (ds for ds in data_sources if ds["id"] == active_id), None
                )
                if source and source not in active_sources:
                    active_sources.append(source)
                    logger.info(
                        f"üìä Active source from panel: {source['name']} ({source['type']})"
                    )

        # If no explicit selection, check for default/primary sources
        if not active_sources:
            # Prioritize Cube.js as default if available
            cube_sources = [ds for ds in data_sources if ds["type"] == "cube"]
            if cube_sources:
                active_sources.append(cube_sources[0])
                logger.info(
                    f"üîÑ Defaulting to primary Cube.js source: {cube_sources[0]['name']}"
                )
            else:
                # Fall back to first available source
                if data_sources:
                    active_sources.append(data_sources[0])
                    logger.info(
                        f"üîÑ Defaulting to first available source: {data_sources[0]['name']} ({data_sources[0]['type']})"
                    )

        return active_sources

    def _build_enhanced_prompt(
        self,
        user_query: str,
        context: Dict,
        strategy: str,
        conversation_history: List[Dict] = None,
        user_selection: Dict = None,
    ) -> str:
        """Build enhanced prompt with full data context and user intent awareness"""

        # Build context-specific instructions
        context_instructions = self._build_context_specific_instructions(
            context, strategy, user_selection
        )

        # Add user intent context
        user_intent = self._analyze_user_intent(user_query)
        intent_context = self._build_intent_context(user_intent, strategy)

        base_prompt = f"""
You are an expert data analyst with access to the following data sources:

{self._format_context_for_prompt(context)}

ANALYSIS STRATEGY: {strategy.upper()}

USER INTENT: {intent_context}

USER QUERY: {user_query}

{context_instructions}

INSTRUCTIONS:
1. Analyze the query in context of available data sources and user intent
2. If using Cube.js: 
   - Use the available measures and dimensions from the schema
   - Generate semantic queries using Cube.js query format
   - Leverage pre-aggregations for performance
   - Focus on business-friendly insights
3. If using database:
   - Generate accurate SQL queries based on schema
   - Consider table relationships and joins
   - Optimize for performance and readability
4. If using files:
   - Analyze column structure and data types
   - Provide statistical insights and patterns
   - Suggest appropriate visualizations
5. Always consider the user's analysis intent and complexity preferences
6. Provide actionable insights and next steps
7. Suggest appropriate chart types and visualizations
8. Be conversational and helpful

Remember: The user has specifically selected or activated certain data sources. Prioritize their selection and provide analysis that matches their intent.
"""

        return base_prompt

    def _build_intent_context(self, user_intent: Dict, strategy: str) -> str:
        """Build context about user's analysis intent"""
        intent_context = []

        if user_intent.get("prefers_cube"):
            intent_context.append("User prefers semantic/business analysis")
        if user_intent.get("prefers_database"):
            intent_context.append("User prefers technical/SQL analysis")
        if user_intent.get("prefers_file"):
            intent_context.append("User prefers simple file analysis")

        intent_context.append(
            f"Analysis type: {user_intent.get('analysis_type', 'general')}"
        )
        intent_context.append(
            f"Complexity level: {user_intent.get('complexity_level', 'basic')}"
        )

        if user_intent.get("time_sensitive"):
            intent_context.append("Time-sensitive analysis requested")
        if user_intent.get("business_focused"):
            intent_context.append("Business-focused insights preferred")

        return (
            "; ".join(intent_context)
            if intent_context
            else "General analysis requested"
        )

    def _build_context_specific_instructions(
        self, context: Dict, strategy: str, user_selection: Dict = None
    ) -> str:
        """Build detailed, strategy-specific instructions for the AI"""
        instructions = []

        # Add user selection context
        if user_selection and user_selection.get("selected_data_source_id"):
            selected_id = user_selection["selected_data_source_id"]
            instructions.append(
                f"IMPORTANT: User has specifically selected data source {selected_id} for analysis."
            )
            instructions.append(
                "Prioritize this selection and provide analysis based on this source."
            )

        if "cube" in strategy.lower():
            instructions.extend(
                [
                    "CUBE.JS ANALYSIS INSTRUCTIONS:",
                    "- Use semantic business language, not technical terms",
                    "- Leverage pre-aggregations for fast performance",
                    "- Focus on business metrics and KPIs",
                    "- Use time dimensions for trend analysis",
                    "- Generate business-friendly insights",
                    "- Suggest dashboard-style visualizations",
                ]
            )

            if context.get("cube_js_sources"):
                cube_source = context["cube_js_sources"][0]
                instructions.append(
                    f"- Available measures: {len(cube_source.get('measures', []))}"
                )
                instructions.append(
                    f"- Available dimensions: {len(cube_source.get('dimensions', []))}"
                )
                instructions.append(
                    f"- Pre-aggregations: {len(cube_source.get('pre_aggregations', []))}"
                )

        elif "database" in strategy.lower():
            instructions.extend(
                [
                    "DATABASE ANALYSIS INSTRUCTIONS:",
                    "- Generate accurate SQL queries based on schema",
                    "- Consider table relationships and joins",
                    "- Optimize for performance",
                    "- Include data validation checks",
                    "- Suggest indexes for performance",
                    "- Provide both simple and complex query options",
                ]
            )

            if context.get("database_sources"):
                db_source = context["database_sources"][0]
                instructions.append(
                    f"- Available tables: {len(db_source.get('tables', {}))}"
                )
                instructions.append(
                    f"- Database type: {db_source.get('type', 'unknown')}"
                )

        elif "file" in strategy.lower():
            instructions.extend(
                [
                    "FILE ANALYSIS INSTRUCTIONS:",
                    "- Analyze column structure and data types",
                    "- Provide statistical summaries",
                    "- Identify patterns and outliers",
                    "- Suggest appropriate chart types",
                    "- Focus on data quality insights",
                    "- Provide data cleaning recommendations",
                ]
            )

            if context.get("file_sources"):
                file_source = context["file_sources"][0]
                instructions.append(
                    f"- File type: {file_source.get('file_type', 'unknown')}"
                )
                instructions.append(
                    f"- Total rows: {file_source.get('row_count', 'unknown')}"
                )
                instructions.append(f"- Columns: {len(file_source.get('columns', []))}")

        # Add general instructions
        instructions.extend(
            [
                "",
                "GENERAL INSTRUCTIONS:",
                "- Always provide actionable insights",
                "- Suggest follow-up questions",
                "- Recommend appropriate visualizations",
                "- Consider data quality and limitations",
                "- Provide business context when possible",
                "- Be conversational and helpful",
            ]
        )

        return "\n".join(instructions)

    async def _execute_cube_analysis(
        self, query: str, context: Dict, prompt: str
    ) -> Dict:
        """Execute analysis using Cube.js semantic layer"""
        try:
            # Use AI to generate Cube.js query
            ai_response = await self._get_ai_response(prompt)

            # Extract Cube.js query parameters
            cube_query = self._extract_cube_query(ai_response, context)

            # Execute against Cube.js API
            cube_result = await self._execute_cube_query(cube_query)

            # Process and enhance results
            enhanced_result = self._enhance_cube_results(cube_result, context)

            # Track executed code for transparency
            executed_code = [
                {
                    "type": "javascript",
                    "language": "javascript",
                    "content": f"// Cube.js Query\nconst query = {{\n  measures: {cube_query.get('measures', [])},\n  dimensions: {cube_query.get('dimensions', [])},\n  filters: {cube_query.get('filters', [])},\n  timeDimensions: {cube_query.get('timeDimensions', [])}\n}};\n\n// Execute query\nconst result = await cubejsApi.load(query);",
                    "description": "Cube.js query execution",
                    "execution_time": 120,
                },
                {
                    "type": "python",
                    "language": "python",
                    "content": "# Cube.js Analysis Pipeline\n# 1. Query Generation\ncube_query = ai_generate_cube_query(query, context)\n# 2. Query Execution\ncube_result = await execute_cube_query(cube_query)\n# 3. Result Enhancement\nenhanced_result = enhance_cube_results(cube_result)\n# 4. Chart Generation\nchart_config = generate_echarts_config(enhanced_result, query)",
                    "description": "Cube.js analysis pipeline execution",
                    "execution_time": 160,
                },
            ]

            return {
                "type": "cube_js_analysis",
                "query": cube_query,
                "results": enhanced_result,
                "ai_insights": ai_response,
                "visualization_config": self._generate_echarts_config(
                    enhanced_result, query
                ),
                "executed_code": executed_code,
            }

        except Exception as e:
            logger.error(f"Cube.js analysis failed: {e}")
            raise

    async def _execute_database_analysis(
        self, query: str, context: Dict, prompt: str
    ) -> Dict:
        """Execute analysis using direct database queries"""
        try:
            # Use AI to generate SQL query
            ai_response = await self._get_ai_response(prompt)

            # Extract and validate SQL
            sql_query = self._extract_sql_query(ai_response, context)
            validated_sql = await self._validate_sql_query(sql_query, context)

            # Execute SQL
            db_result = await self._execute_sql_query(validated_sql, context)

            # Process results
            enhanced_result = self._enhance_database_results(db_result, context)

            # Track executed code for transparency
            executed_code = [
                {
                    "type": "sql",
                    "language": "sql",
                    "content": validated_sql,
                    "description": "AI-generated SQL query",
                    "execution_time": 200,  # Mock execution time
                },
                {
                    "type": "python",
                    "language": "python",
                    "content": "# Database Analysis Pipeline\n# 1. SQL Generation\nsql_query = ai_generate_sql(query, context)\n# 2. SQL Validation\nvalidated_sql = validate_sql(sql_query, context)\n# 3. Query Execution\ndb_result = execute_sql(validated_sql)\n# 4. Result Enhancement\nenhanced_result = enhance_results(db_result)\n# 5. Chart Generation\nchart_config = generate_echarts_config(enhanced_result, query)",
                    "description": "Database analysis pipeline execution",
                    "execution_time": 180,
                },
            ]

            return {
                "type": "database_analysis",
                "sql_query": validated_sql,
                "results": enhanced_result,
                "ai_insights": ai_response,
                "visualization_config": self._generate_echarts_config(
                    enhanced_result, query
                ),
                "executed_code": executed_code,
            }

        except Exception as e:
            logger.error(f"Database analysis failed: {e}")
            raise

    def _generate_echarts_config(self, data: Dict, query: str) -> Dict:
        """Generate ECharts configuration based on data and query"""
        try:
            # Analyze data structure and query intent
            chart_type = self._determine_chart_type(data, query)

            # Generate appropriate ECharts config
            if chart_type == "line":
                return self._generate_line_chart_config(data, {"query": query})
            elif chart_type == "bar":
                return self._generate_bar_chart_config(data, {"query": query})
            elif chart_type == "pie":
                return self._generate_pie_chart_config(data, {"query": query})
            elif chart_type == "scatter":
                return self._generate_scatter_chart_config(data, {"query": query})
            else:
                return self._generate_table_config(data, {"query": query})

        except Exception as e:
            logger.warning(f"Failed to generate ECharts config: {e}")
            return {"type": "table", "data": data}

    def _determine_chart_type(self, data: Dict, query: str) -> str:
        """Determine the best chart type based on data and query"""
        query_lower = query.lower()

        # Time-based queries
        if any(
            word in query_lower
            for word in ["trend", "over time", "timeline", "history"]
        ):
            return "line"

        # Comparison queries
        if any(
            word in query_lower for word in ["compare", "vs", "versus", "difference"]
        ):
            return "bar"

        # Distribution queries
        if any(
            word in query_lower for word in ["distribution", "percentage", "proportion"]
        ):
            return "pie"

        # Correlation queries
        if any(
            word in query_lower for word in ["correlation", "relationship", "scatter"]
        ):
            return "scatter"

        # Default to table for complex data
        return "table"

    async def _execute_file_analysis(
        self, query: str, context: Dict, prompt: str
    ) -> Dict:
        """Execute analysis using uploaded file data"""
        try:
            logger.info(f"üîç Executing file analysis for query: {query}")
            logger.info(f"üîç Context keys: {list(context.keys())}")
            logger.info(f"üîç Context content: {context}")

            # Get file data source from context
            file_sources = context.get("file_sources", [])
            logger.info(f"üîç File sources found: {len(file_sources)}")
            if not file_sources:
                # Try to find any data sources in the context
                all_sources = []
                if "data_source_inventory" in context:
                    all_sources = context["data_source_inventory"]
                elif "data_sources" in context:
                    all_sources = context["data_sources"]

                logger.info(f"üîç All sources in context: {all_sources}")

                # Look for file-type sources
                file_sources = [s for s in all_sources if s.get("type") == "file"]
                logger.info(f"üîç File sources after filtering: {len(file_sources)}")

                if not file_sources:
                    raise Exception("No file sources available for analysis")

            file_source = file_sources[0]  # Use first file source
            file_source["id"]

            # Use schema-based analysis directly since we have all the metadata we need
            logger.info(f"üìä Using schema-based analysis for {file_source['name']}")
            # Generate sample data based on schema for chart generation
            file_data = self._generate_sample_data_from_schema(file_source)

            if not file_data:
                raise Exception(
                    f"No data available for analysis in {file_source['name']}"
                )

            # Analyze the actual data
            analysis_result = await self._analyze_file_data(
                query, file_data, file_source
            )

            # Generate chart configuration based on analysis
            chart_config = await self._generate_chart_from_analysis(
                analysis_result, file_data
            )

            # Track executed code for transparency
            executed_code = [
                {
                    "type": "python",
                    "language": "python",
                    "content": f"# Schema-Based Data Analysis Pipeline\n# 1. Schema Analysis\nfile_schema = analyze_file_schema('{file_source['name']}')\n# 2. Sample Data Generation\nfile_data = generate_sample_data_from_schema(file_schema)\n# 3. Data Analysis\nanalysis_result = await analyze_file_data(query, file_data)\n# 4. Chart Generation\nchart_config = await generate_chart_from_analysis(analysis_result, file_data)",
                    "description": "Schema-based data analysis pipeline execution",
                    "execution_time": 120,
                },
                {
                    "type": "json",
                    "language": "json",
                    "content": json.dumps(
                        {
                            "data_summary": {
                                "total_rows": file_source.get("row_count", 0),
                                "columns": [
                                    col["name"]
                                    for col in file_source.get("schema", {}).get(
                                        "columns", []
                                    )
                                ],
                                "file_source": file_source["name"],
                                "file_size": file_source.get("size", 0),
                                "analysis_method": "schema_based",
                            }
                        },
                        indent=2,
                    ),
                    "description": "File schema and metadata analysis",
                    "execution_time": 30,
                },
            ]

            return {
                "type": "file_analysis",
                "query": query,
                "results": {
                    "data": file_data,
                    "total_rows": len(file_data),
                    "insights": analysis_result,
                    "chart_config": chart_config,
                },
                "ai_insights": analysis_result,
                "visualization_config": chart_config,
                "executed_code": executed_code,
            }

        except Exception as e:
            logger.error(f"File analysis failed: {e}")
            raise

    async def _analyze_file_data(
        self, query: str, data: List[Dict], file_source: Dict
    ) -> Dict:
        """Analyze actual file data based on user query"""
        try:
            # Create analysis prompt with actual data context
            data_summary = self._create_data_summary(data, file_source)

            analysis_prompt = f"""
            Analyze this data based on the user query: "{query}"
            
            Data Summary:
            {data_summary}
            
            Sample Data (first 5 rows):
            {data[:5]}
            
            Please provide:
            1. Key insights from the data
            2. Specific findings related to the query
            3. Data patterns and trends
            4. Recommendations for visualization
            5. Suggested chart types with reasoning
            
            Be specific and actionable based on the actual data provided.
            """

            # Get AI analysis
            ai_response = await self._get_ai_response(analysis_prompt)

            return {
                "query": query,
                "data_summary": data_summary,
                "ai_analysis": ai_response,
                "data_insights": self._extract_data_insights(data, query),
            }

        except Exception as e:
            logger.error(f"Data analysis failed: {e}")
            return {"error": str(e)}

    def _create_data_summary(self, data: List[Dict], file_source: Dict) -> str:
        """Create summary of file data for analysis"""
        if not data:
            return "No data available"

        columns = list(data[0].keys()) if data else []
        row_count = len(data)

        # Get column statistics
        column_stats = {}
        for col in columns:
            if data:
                values = [row.get(col) for row in data if row.get(col) is not None]
                if values:
                    if isinstance(values[0], (int, float)):
                        column_stats[col] = {
                            "type": "numeric",
                            "min": min(values),
                            "max": max(values),
                            "mean": sum(values) / len(values),
                            "count": len(values),
                        }
                    else:
                        column_stats[col] = {
                            "type": "categorical",
                            "unique_values": len(set(values)),
                            "sample_values": list(set(values))[:5],
                        }

        summary = f"""
        File: {file_source["name"]}
        Total Rows: {row_count}
        Columns: {len(columns)}
        
        Column Details:
        """

        for col in columns:
            if col in column_stats:
                stats = column_stats[col]
                if stats["type"] == "numeric":
                    summary += f"\n- {col}: numeric (min: {stats['min']}, max: {stats['max']}, mean: {stats['mean']:.2f})"
                else:
                    summary += f"\n- {col}: categorical ({stats['unique_values']} unique values)"

        return summary

    def _extract_data_insights(self, data: List[Dict], query: str) -> Dict:
        """Extract basic insights from data"""
        if not data:
            return {}

        insights = {
            "total_records": len(data),
            "columns": list(data[0].keys()) if data else [],
            "data_types": {},
            "basic_stats": {},
        }

        # Analyze data types and basic statistics
        for col in insights["columns"]:
            if data:
                values = [row.get(col) for row in data if row.get(col) is not None]
                if values:
                    if isinstance(values[0], (int, float)):
                        insights["data_types"][col] = "numeric"
                        insights["basic_stats"][col] = {
                            "min": min(values),
                            "max": max(values),
                            "mean": sum(values) / len(values),
                        }
                    else:
                        insights["data_types"][col] = "categorical"
                        insights["basic_stats"][col] = {
                            "unique_count": len(set(values))
                        }

        return insights

    async def _generate_chart_from_analysis(
        self, analysis: Dict, data: List[Dict]
    ) -> Dict:
        """Generate ECharts configuration based on analysis and data"""
        try:
            if not data:
                return {"type": "table", "data": []}

            # Determine chart type based on query and data
            chart_type = self._determine_chart_type(analysis.get("query", ""), data)

            # Generate chart configuration
            if chart_type == "bar":
                return self._generate_bar_chart_config(data, analysis)
            elif chart_type == "line":
                return self._generate_line_chart_config(data, analysis)
            elif chart_type == "area":
                return self._generate_area_chart_config(data, analysis)
            elif chart_type == "pie":
                return self._generate_pie_chart_config(data, analysis)
            elif chart_type == "scatter":
                return self._generate_scatter_chart_config(data, analysis)
            elif chart_type == "heatmap":
                return self._generate_heatmap_chart_config(data, analysis)
            elif chart_type == "radar":
                return self._generate_radar_chart_config(data, analysis)
            else:
                return self._generate_table_config(data, analysis)

        except Exception as e:
            logger.error(f"Chart generation failed: {e}")
            return {"type": "table", "data": [], "error": str(e)}

    def _determine_chart_type(self, query: str, data: List[Dict]) -> str:
        """Determine best chart type based on query and data"""
        query_lower = query.lower()

        # Time-based analysis
        if any(
            word in query_lower
            for word in ["trend", "over time", "growth", "change", "evolution"]
        ):
            return "line"

        # Comparison analysis
        if any(
            word in query_lower
            for word in ["compare", "vs", "versus", "difference", "ranking"]
        ):
            return "bar"

        # Distribution analysis
        if any(
            word in query_lower
            for word in ["distribution", "percentage", "proportion", "breakdown"]
        ):
            return "pie"

        # Correlation analysis
        if any(
            word in query_lower
            for word in ["correlation", "relationship", "scatter", "pattern"]
        ):
            return "scatter"

        # Cumulative/stacked analysis
        if any(
            word in query_lower for word in ["cumulative", "stacked", "area", "fill"]
        ):
            return "area"

        # Multi-dimensional comparison
        if any(
            word in query_lower
            for word in ["dimensions", "factors", "criteria", "radar"]
        ):
            return "radar"

        # Matrix/correlation matrix
        if any(
            word in query_lower
            for word in ["matrix", "heatmap", "correlation matrix", "grid"]
        ):
            return "heatmap"

        # Default based on data characteristics
        if data and len(data[0]) >= 2:
            numeric_cols = [
                col for col in data[0].keys() if isinstance(data[0][col], (int, float))
            ]
            categorical_cols = [
                col
                for col in data[0].keys()
                if not isinstance(data[0][col], (int, float))
            ]

            if len(numeric_cols) >= 3 and len(categorical_cols) >= 2:
                return "heatmap"  # Good for correlation matrices
            elif len(numeric_cols) >= 2:
                return "scatter"  # Good for correlations
            elif len(numeric_cols) >= 1 and len(categorical_cols) >= 1:
                return "bar"  # Good for comparisons
            elif len(numeric_cols) >= 1:
                return "line"  # Good for trends

        return "table"

    def _generate_bar_chart_config(self, data: List[Dict], analysis: Dict) -> Dict:
        """Generate ECharts bar chart configuration"""
        if not data:
            return {"type": "bar", "data": []}

        # Find numeric and categorical columns
        numeric_cols = []
        categorical_cols = []

        for col in data[0].keys():
            if data:
                sample_value = data[0][col]
                if isinstance(sample_value, (int, float)):
                    numeric_cols.append(col)
                else:
                    categorical_cols.append(col)

        if not numeric_cols or not categorical_cols:
            return {"type": "table", "data": data}

        # Use first numeric column as values, first categorical as categories
        value_col = numeric_cols[0]
        category_col = categorical_cols[0]

        # Aggregate data by category
        aggregated = {}
        for row in data:
            category = str(row.get(category_col, "Unknown"))
            value = float(row.get(value_col, 0))
            aggregated[category] = aggregated.get(category, 0) + value

        # Prepare chart data
        categories = list(aggregated.keys())
        values = list(aggregated.values())

        return {
            "type": "bar",
            "title": f"{value_col} by {category_col}",
            "xAxis": {"type": "category", "data": categories},
            "yAxis": {"type": "value"},
            "series": [{"type": "bar", "data": values, "name": value_col}],
            "tooltip": {"trigger": "axis"},
            "data": data,  # Include raw data for reference
        }

    def _generate_line_chart_config(self, data: List[Dict], analysis: Dict) -> Dict:
        """Generate ECharts line chart configuration"""
        if not data:
            return {"type": "line", "data": []}

        # Find date/time and numeric columns
        date_cols = []
        numeric_cols = []

        for col in data[0].keys():
            if data:
                sample_value = data[0][col]
                if isinstance(sample_value, (int, float)):
                    numeric_cols.append(col)
                elif isinstance(sample_value, str) and any(
                    word in col.lower() for word in ["date", "time", "year", "month"]
                ):
                    date_cols.append(col)

        if not numeric_cols:
            return {"type": "table", "data": data}

        # Use first numeric column as values
        value_col = numeric_cols[0]
        x_col = date_cols[0] if date_cols else list(data[0].keys())[0]

        # Sort data by x-axis
        sorted_data = sorted(data, key=lambda x: str(x.get(x_col, "")))

        x_values = [str(row.get(x_col, "")) for row in sorted_data]
        y_values = [float(row.get(value_col, 0)) for row in sorted_data]

        return {
            "type": "line",
            "title": f"{value_col} over {x_col}",
            "xAxis": {"type": "category", "data": x_values},
            "yAxis": {"type": "value"},
            "series": [{"type": "line", "data": y_values, "name": value_col}],
            "tooltip": {"trigger": "axis"},
            "data": data,
        }

    def _generate_pie_chart_config(self, data: List[Dict], analysis: Dict) -> Dict:
        """Generate ECharts pie chart configuration"""
        if not data:
            return {"type": "pie", "data": []}

        # Find categorical and numeric columns
        categorical_cols = []
        numeric_cols = []

        for col in data[0].keys():
            if data:
                sample_value = data[0][col]
                if isinstance(sample_value, (int, float)):
                    numeric_cols.append(col)
                else:
                    categorical_cols.append(col)

        if not numeric_cols or not categorical_cols:
            return {"type": "table", "data": data}

        # Use first numeric column as values, first categorical as categories
        value_col = numeric_cols[0]
        category_col = categorical_cols[0]

        # Aggregate data by category
        aggregated = {}
        for row in data:
            category = str(row.get(category_col, "Unknown"))
            value = float(row.get(value_col, 0))
            aggregated[category] = aggregated.get(category, 0) + value

        # Prepare pie chart data
        pie_data = [{"name": k, "value": v} for k, v in aggregated.items()]

        return {
            "type": "pie",
            "title": f"{value_col} by {category_col}",
            "data": pie_data,
            "radius": "50%",
        }

    def _generate_scatter_chart_config(self, data: List[Dict], analysis: Dict) -> Dict:
        """Generate ECharts scatter chart configuration"""
        if not data:
            return {"type": "scatter", "data": []}

        # Find numeric columns
        numeric_cols = []
        for col in data[0].keys():
            if data:
                sample_value = data[0][col]
                if isinstance(sample_value, (int, float)):
                    numeric_cols.append(col)

        if len(numeric_cols) < 2:
            return {"type": "table", "data": data}

        # Use first two numeric columns for x and y
        x_col = numeric_cols[0]
        y_col = numeric_cols[1]

        # Prepare scatter data
        scatter_data = []
        for row in data:
            x_val = row.get(x_col)
            y_val = row.get(y_col)
            if x_val is not None and y_val is not None:
                scatter_data.append([float(x_val), float(y_val)])

        return {
            "type": "scatter",
            "title": f"{y_col} vs {x_col}",
            "xAxis": {"type": "value", "name": x_col},
            "yAxis": {"type": "value", "name": y_col},
            "series": [{"type": "scatter", "data": scatter_data, "symbolSize": 8}],
            "tooltip": {"trigger": "item"},
            "data": data,
        }

    def _generate_table_config(self, data: List[Dict], analysis: Dict) -> Dict:
        """Generate table configuration"""
        if not data:
            return {"type": "table", "data": []}

        columns = list(data[0].keys()) if data else []

        return {
            "type": "table",
            "columns": columns,
            "data": data,
            "title": "Data Table",
        }

    def _generate_area_chart_config(self, data: List[Dict], analysis: Dict) -> Dict:
        """Generate ECharts area chart configuration for cumulative/stacked data"""
        if not data:
            return {"type": "area", "data": []}

        # Find date/time and numeric columns
        date_cols = []
        numeric_cols = []

        for col in data[0].keys():
            if data:
                sample_value = data[0][col]
                if isinstance(sample_value, (int, float)):
                    numeric_cols.append(col)
                elif isinstance(sample_value, str) and any(
                    word in col.lower() for word in ["date", "time", "year", "month"]
                ):
                    date_cols.append(col)

        if not numeric_cols:
            return {"type": "table", "data": data}

        # Use first numeric column as values
        value_col = numeric_cols[0]
        x_col = date_cols[0] if date_cols else list(data[0].keys())[0]

        # Sort data by x-axis
        sorted_data = sorted(data, key=lambda x: str(x.get(x_col, "")))

        x_values = [str(row.get(x_col, "")) for row in sorted_data]
        y_values = [float(row.get(value_col, 0)) for row in sorted_data]

        return {
            "type": "area",
            "title": f"Cumulative {value_col} over {x_col}",
            "xAxis": {"type": "category", "data": x_values},
            "yAxis": {"type": "value"},
            "series": [
                {
                    "type": "line",
                    "data": y_values,
                    "name": value_col,
                    "areaStyle": {"opacity": 0.6, "color": "#91cc75"},
                    "smooth": True,
                }
            ],
            "tooltip": {"trigger": "axis"},
            "data": data,
        }

    def _generate_heatmap_chart_config(self, data: List[Dict], analysis: Dict) -> Dict:
        """Generate ECharts heatmap configuration for correlation matrices"""
        if not data:
            return {"type": "heatmap", "data": []}

        # Find numeric columns for heatmap
        numeric_cols = []
        for col in data[0].keys():
            if data:
                sample_value = data[0][col]
                if isinstance(sample_value, (int, float)):
                    numeric_cols.append(col)

        if len(numeric_cols) < 2:
            return {"type": "table", "data": data}

        # Create correlation matrix data
        heatmap_data = []
        for i, col1 in enumerate(numeric_cols):
            for j, col2 in enumerate(numeric_cols):
                if i <= j:  # Upper triangle
                    # Calculate correlation or use sample values
                    value = (
                        data[0].get(col1, 0)
                        if i == j
                        else data[0].get(col1, 0) + data[0].get(col2, 0)
                    )
                    heatmap_data.append([i, j, value])

        return {
            "type": "heatmap",
            "title": "Correlation Matrix",
            "xAxis": {"type": "category", "data": numeric_cols},
            "yAxis": {"type": "category", "data": numeric_cols},
            "visualMap": {
                "min": 0,
                "max": 100,
                "calculable": True,
                "orient": "horizontal",
                "left": "center",
                "bottom": "15%",
            },
            "series": [
                {
                    "type": "heatmap",
                    "data": heatmap_data,
                    "label": {"show": True},
                    "emphasis": {
                        "itemStyle": {
                            "shadowBlur": 10,
                            "shadowColor": "rgba(0, 0, 0, 0.5)",
                        }
                    },
                }
            ],
            "data": data,
        }

    def _generate_radar_chart_config(self, data: List[Dict], analysis: Dict) -> Dict:
        """Generate ECharts radar chart configuration for multi-dimensional analysis"""
        if not data:
            return {"type": "radar", "data": []}

        # Find numeric columns for radar dimensions
        numeric_cols = []
        for col in data[0].keys():
            if data:
                sample_value = data[0][col]
                if isinstance(sample_value, (int, float)):
                    numeric_cols.append(col)

        if len(numeric_cols) < 3:
            return {"type": "table", "data": data}

        # Limit to top 6 dimensions for readability
        top_cols = numeric_cols[:6]

        # Create radar indicator
        indicator = [{"name": col, "max": 100} for col in top_cols]

        # Create radar data
        radar_data = []
        for row in data[:5]:  # Limit to 5 rows for clarity
            values = [float(row.get(col, 0)) for col in top_cols]
            radar_data.append({"value": values, "name": f"Row {len(radar_data) + 1}"})

        return {
            "type": "radar",
            "title": "Multi-Dimensional Analysis",
            "radar": {"indicator": indicator, "radius": "65%"},
            "series": [
                {"type": "radar", "data": radar_data, "areaStyle": {"opacity": 0.3}}
            ],
            "tooltip": {"trigger": "item"},
            "data": data,
        }

    def _generate_sample_data_from_schema(self, file_source: Dict) -> List[Dict]:
        """Generate sample data based on file schema for chart generation"""
        try:
            schema = file_source.get("schema", {})
            columns = schema.get("columns", [])

            if not columns:
                return []

            # Generate sample data based on column types and statistics
            sample_data = []
            row_count = file_source.get("row_count", 10)

            for i in range(min(row_count, 20)):  # Limit to 20 rows for performance
                row = {}
                for col in columns:
                    col_name = col.get("name", "")
                    col_type = col.get("type", "string")
                    stats = col.get("statistics", {})

                    if col_type in ["number", "integer", "float"]:
                        # Generate numeric data
                        if "min" in stats and "max" in stats:
                            import random

                            row[col_name] = round(
                                random.uniform(stats["min"], stats["max"]), 2
                            )
                        else:
                            row[col_name] = i + 1
                    elif col_type == "string":
                        # Generate categorical data
                        if "unique_count" in stats and stats["unique_count"] > 0:
                            # Use sample values if available
                            sample_values = stats.get("sample_values", [])
                            if sample_values:
                                row[col_name] = sample_values[i % len(sample_values)]
                            else:
                                # Generate generic values
                                categories = [
                                    "Category A",
                                    "Category B",
                                    "Category C",
                                    "Category D",
                                    "Category E",
                                ]
                                row[col_name] = categories[i % len(categories)]
                        else:
                            row[col_name] = f"Value {i + 1}"
                    else:
                        # Default string value
                        row[col_name] = f"Data {i + 1}"

                sample_data.append(row)

            logger.info(
                f"üìä Generated {len(sample_data)} sample rows from schema for {file_source['name']}"
            )
            return sample_data

        except Exception as e:
            logger.error(f"Failed to generate sample data: {e}")
            return []

    async def _get_file_context(self, source: Dict) -> Dict:
        """Get detailed context from file source without actual data values"""
        try:
            # Use the source data directly instead of making HTTP calls
            schema_data = source.get("schema", {})

            # Build comprehensive file context
            file_context = {
                "id": source["id"],
                "name": source["name"],
                "size": source.get("size", "unknown"),
                "row_count": source.get("row_count", 0),
                "columns": schema_data.get("columns", []),
                "file_type": source["name"].split(".")[-1].lower()
                if "." in source["name"]
                else "unknown",
                "schema": schema_data,
                "analysis_capabilities": {
                    "can_aggregate": self._has_numeric_columns(schema_data),
                    "can_dimension": len(schema_data.get("columns", [])) > 0,
                    "can_filter": True,
                    "can_time_series": self._has_time_columns_file(schema_data),
                    "total_columns": len(schema_data.get("columns", [])),
                    "data_types": self._get_file_data_types(schema_data),
                },
                "query_templates": self._generate_file_query_templates(schema_data),
                "schema_summary": self._build_file_schema_summary(source, schema_data),
            }

            return file_context

        except Exception as e:
            logger.warning(f"Failed to get file context for {source['id']}: {e}")
            return {"id": source["id"], "name": source["name"], "error": str(e)}

    async def _get_database_context(self, source: Dict) -> Dict:
        """Get detailed context from database source"""
        try:
            source_id = source["id"]
            schema_data = await self._fetch_database_schema(source_id)

            # Build comprehensive database context
            db_context = {
                "id": source["id"],
                "name": source["name"],
                "type": source.get("type", "database"),
                "tables": schema_data.get("tables", {}),
                "schemas": schema_data.get("schemas", []),
                "connection_info": source.get("config", {}),
                "analysis_capabilities": {
                    "can_aggregate": True,
                    "can_dimension": True,
                    "can_filter": True,
                    "can_time_series": True,
                    "can_join": True,
                    "total_tables": len(schema_data.get("tables", {})),
                    "total_schemas": len(schema_data.get("schemas", [])),
                    "supported_operations": [
                        "sql_queries",
                        "complex_joins",
                        "stored_procedures",
                        "views",
                    ],
                },
                "query_templates": self._generate_database_query_templates(schema_data),
                "schema_summary": self._build_database_schema_summary(
                    source, schema_data
                ),
            }

            return db_context

        except Exception as e:
            logger.warning(f"Failed to get database context for {source['id']}: {e}")
            return {"id": source["id"], "name": source["name"], "error": str(e)}

    async def _get_cube_context(self, source: Dict) -> Dict:
        """Get detailed context from Cube.js source"""
        try:
            endpoint = source.get("config", {}).get("endpoint", "http://localhost:4000")
            schema_data = await self._fetch_cube_schema(endpoint)

            # Build comprehensive Cube.js context
            cube_context = {
                "id": source["id"],
                "name": source["name"],
                "type": "cube",
                "endpoint": endpoint,
                "cubes": schema_data.get("cubes", []),
                "measures": self._extract_cube_measures(schema_data.get("cubes", [])),
                "dimensions": self._extract_cube_dimensions(
                    schema_data.get("cubes", [])
                ),
                "segments": self._extract_cube_segments(schema_data.get("cubes", [])),
                "pre_aggregations": self._extract_pre_aggregations(
                    schema_data.get("cubes", [])
                ),
                "analysis_capabilities": {
                    "can_aggregate": True,
                    "can_dimension": True,
                    "can_filter": True,
                    "can_time_series": True,
                    "can_join": True,
                    "total_cubes": len(schema_data.get("cubes", [])),
                    "total_measures": len(
                        self._extract_cube_measures(schema_data.get("cubes", []))
                    ),
                    "total_dimensions": len(
                        self._extract_cube_dimensions(schema_data.get("cubes", []))
                    ),
                    "supported_operations": [
                        "semantic_queries",
                        "pre_aggregations",
                        "time_granularity",
                        "business_logic",
                    ],
                },
                "query_templates": self._generate_cube_query_templates(schema_data),
                "schema_summary": self._build_cube_schema_summary(source, schema_data),
            }

            return cube_context

        except Exception as e:
            logger.warning(f"Failed to get Cube.js context for {source['id']}: {e}")
            return {"id": source["id"], "name": source["name"], "error": str(e)}

    async def _fetch_cube_schema(self, endpoint: str) -> Dict:
        """Fetch schema from Cube.js API"""
        try:
            import httpx

            async with httpx.AsyncClient() as client:
                response = await client.get(f"{endpoint}/cubejs-api/v1/meta")
                return response.json()
        except Exception as e:
            logger.error(f"Failed to fetch Cube.js schema: {e}")
            return {"cubes": []}

    async def _fetch_database_schema(self, source_id: str) -> Dict:
        """Fetch schema from database service"""
        try:
            import httpx

            async with httpx.AsyncClient() as client:
                response = await client.get(
                    f"http://localhost:8000/data/sources/{source_id}/schema"
                )
                return response.json().get("schema", {})
        except Exception as e:
            logger.error(f"Failed to fetch database schema: {e}")
            return {"tables": {}, "schemas": []}

    def _extract_cube_measures(self, cubes: List[Dict]) -> List[Dict]:
        """Extract all measures from Cube.js cubes"""
        measures = []
        for cube in cubes:
            measures.extend(cube.get("measures", []))
        return measures

    def _extract_cube_dimensions(self, cubes: List[Dict]) -> List[Dict]:
        """Extract all dimensions from Cube.js cubes"""
        dimensions = []
        for cube in cubes:
            dimensions.extend(cube.get("dimensions", []))
        return dimensions

    def _extract_cube_segments(self, cubes: List[Dict]) -> List[Dict]:
        """Extract all segments from Cube.js cubes"""
        segments = []
        for cube in cubes:
            segments.extend(cube.get("segments", []))
        return segments

    def _extract_pre_aggregations(self, cubes: List[Dict]) -> List[Dict]:
        """Extract all pre-aggregations from Cube.js cubes"""
        pre_aggs = []
        for cube in cubes:
            pre_aggs.extend(cube.get("preAggregations", []))
        return pre_aggs

    def _generate_database_query_templates(self, schema_data: Dict) -> List[str]:
        """Generate sample SQL query templates for database"""
        templates = []
        tables = schema_data.get("tables", {})

        for table_name, table_info in tables.items():
            if table_info.get("columns"):
                columns = [col["name"] for col in table_info["columns"]]
                templates.append(
                    f"SELECT {', '.join(columns[:3])} FROM {table_name} LIMIT 10"
                )

        return templates[:5]  # Return max 5 templates

    def _generate_cube_query_templates(self, schema_data: Dict) -> List[str]:
        """Generate sample Cube.js query templates"""
        templates = []
        cubes = schema_data.get("cubes", [])

        for cube in cubes[:3]:  # Max 3 cubes
            if cube.get("measures") and cube.get("dimensions"):
                measure = cube["measures"][0]["name"] if cube["measures"] else "count"
                dimension = (
                    cube["dimensions"][0]["name"] if cube["dimensions"] else "id"
                )
                templates.append(
                    f"{{ measures: ['{measure}'], dimensions: ['{dimension}'] }}"
                )

        return templates

    def _build_database_schema_summary(self, source: Dict, schema_data: Dict) -> str:
        """Build human-readable database schema summary"""
        tables = schema_data.get("tables", {})
        schemas = schema_data.get("schemas", [])

        summary = f"Database '{source['name']}' with {len(tables)} tables across {len(schemas)} schemas"
        if tables:
            table_names = list(tables.keys())[:3]
            summary += f". Key tables: {', '.join(table_names)}"

        return summary

    def _build_cube_schema_summary(self, source: Dict, schema_data: Dict) -> str:
        """Build human-readable Cube.js schema summary"""
        cubes = schema_data.get("cubes", [])
        measures = self._extract_cube_measures(cubes)
        dimensions = self._extract_cube_dimensions(cubes)

        summary = f"Cube.js '{source['name']}' with {len(cubes)} semantic models"
        if measures and dimensions:
            summary += f", {len(measures)} metrics, {len(dimensions)} dimensions"

        return summary

    def _build_context_summary(self, context: Dict) -> str:
        """Build human-readable summary of available data context"""
        summary_parts = []

        if context["cube_js_sources"]:
            cube_count = len(context["cube_js_sources"])
            measure_count = len(context["available_metrics"])
            dimension_count = len(context["available_dimensions"])
            summary_parts.append(
                f"üìä {cube_count} Cube.js semantic models with {measure_count} metrics and {dimension_count} dimensions"
            )

        if context["database_sources"]:
            db_count = len(context["database_sources"])
            table_count = len(context["table_schemas"])
            summary_parts.append(
                f"üóÑÔ∏è {db_count} database connections with {table_count} tables"
            )

        if context["file_sources"]:
            file_count = len(context["file_sources"])
            summary_parts.append(f"üìÅ {file_count} uploaded data files")

        if not summary_parts:
            return "No data sources available for analysis"

        return " | ".join(summary_parts)

    def _format_context_for_prompt(self, context: Dict) -> str:
        """Format context for AI prompt"""
        prompt_parts = []

        # Cube.js sources
        if context["cube_js_sources"]:
            prompt_parts.append("CUBE.JS SEMANTIC MODELS:")
            for source in context["cube_js_sources"]:
                if "error" not in source:
                    prompt_parts.append(
                        f"- {source['name']}: {len(source.get('cubes', []))} cubes"
                    )
                    if source.get("measures"):
                        prompt_parts.append(
                            f"  Measures: {', '.join([m.get('title', m.get('name', '')) for m in source['measures'][:5]])}"
                        )
                    if source.get("dimensions"):
                        prompt_parts.append(
                            f"  Dimensions: {', '.join([d.get('title', d.get('name', '')) for d in source['dimensions'][:5]])}"
                        )

        # Database sources
        if context["database_sources"]:
            prompt_parts.append("DATABASE SOURCES:")
            for source in context["database_sources"]:
                if "error" not in source:
                    prompt_parts.append(
                        f"- {source['name']} ({source.get('type', 'unknown')})"
                    )
                    tables = source.get("tables", {})
                    if tables:
                        prompt_parts.append(
                            f"  Tables: {', '.join(list(tables.keys())[:5])}"
                        )

        # File sources
        if context["file_sources"]:
            prompt_parts.append("UPLOADED FILES:")
            for source in context["file_sources"]:
                prompt_parts.append(f"- {source['name']}")

        return "\n".join(prompt_parts)

    def _format_conversation_history(self, history: List[Dict]) -> str:
        """Format conversation history for context"""
        if not history:
            return "No previous conversation context"

        formatted = []
        for msg in history[-5:]:  # Last 5 messages
            role = msg.get("role", "unknown")
            content = msg.get("content", "")[:200]  # Truncate long messages
            formatted.append(f"{role.upper()}: {content}")

        return "\n".join(formatted)

    async def _get_ai_response(self, prompt: str) -> str:
        """Get AI response using the configured model"""
        try:
            logger.info(f"üîç Getting AI response for prompt: {prompt[:100]}...")
            logger.info(f"üîç Using LiteLLM service: {self.litellm_service}")

            # Use the correct method call with prompt parameter
            response = await self.litellm_service.generate_completion(
                prompt=prompt, max_tokens=2000
            )

            logger.info(f"üîç LiteLLM response: {response}")

            content = response.get("content", "")
            if not content:
                logger.error(f"‚ùå Empty content from LiteLLM service: {response}")
                raise Exception("Empty content from AI service")

            return content
        except Exception as e:
            logger.error(f"‚ùå AI response failed: {e}")
            raise

    def _get_current_model(self) -> str:
        """Get the currently configured AI model"""
        try:
            self.litellm_service.get_model_info()
            # Use Azure GPT-5 Mini as default
            return "azure_gpt5_mini"  # model_info.get("model_name", "gpt-4")
        except:
            return "azure_gpt5_mini"

    async def _execute_cube_query(self, query_params: Dict) -> Dict:
        """Execute query against Cube.js API"""
        try:
            import httpx

            async with httpx.AsyncClient() as client:
                response = await client.post(
                    "http://localhost:4000/cubejs-api/v1/load", json=query_params
                )
                return response.json()
        except Exception as e:
            logger.error(f"Cube.js query execution failed: {e}")
            raise

    async def _execute_sql_query(self, sql: str, context: Dict) -> Dict:
        """Execute SQL query against database"""
        try:
            import httpx

            async with httpx.AsyncClient() as client:
                response = await client.post(
                    "http://localhost:8000/data/execute",
                    json={
                        "sql": sql,
                        "source_id": context.get("database_sources", [{}])[0].get("id"),
                    },
                )
                return response.json()
        except Exception as e:
            logger.error(f"SQL execution failed: {e}")
            raise

    def _extract_cube_query(self, ai_response: str, context: Dict) -> Dict:
        """Extract Cube.js query parameters from AI response"""
        # This is a simplified extraction - in production, you'd want more sophisticated parsing
        try:
            # Look for common Cube.js query patterns in the AI response
            if "measures" in ai_response.lower():
                # Extract measures mentioned
                measures = [
                    m["name"]
                    for m in context.get("available_metrics", [])
                    if m["name"].lower() in ai_response.lower()
                ]

                # Extract dimensions mentioned
                dimensions = [
                    d["name"]
                    for d in context.get("available_dimensions", [])
                    if d["name"].lower() in ai_response.lower()
                ]

                return {
                    "measures": measures[:3],  # Limit to 3 measures
                    "dimensions": dimensions[:2],  # Limit to 2 dimensions
                    "timeDimensions": [],
                    "filters": [],
                }
            else:
                # Default query
                return {
                    "measures": [
                        context.get("available_metrics", [{}])[0].get("name", "count")
                    ],
                    "dimensions": [],
                    "timeDimensions": [],
                    "filters": [],
                }
        except Exception as e:
            logger.warning(f"Failed to extract Cube.js query: {e}")
            return {
                "measures": ["count"],
                "dimensions": [],
                "timeDimensions": [],
                "filters": [],
            }

    def _extract_sql_query(self, ai_response: str, context: Dict) -> str:
        """Extract SQL query from AI response"""
        # Look for SQL code blocks
        import re

        sql_match = re.search(
            r"```sql\s*(.*?)\s*```", ai_response, re.DOTALL | re.IGNORECASE
        )
        if sql_match:
            return sql_match.group(1).strip()

        # Fallback: generate simple SELECT query
        tables = list(context.get("table_schemas", {}).keys())
        if tables:
            return f"SELECT * FROM {tables[0]} LIMIT 100"

        return "SELECT 1"

    async def _validate_sql_query(self, sql: str, context: Dict) -> str:
        """Validate and sanitize SQL query"""
        # Basic SQL validation - in production, use proper SQL parsing
        sql_upper = sql.upper()

        # Check for dangerous operations
        dangerous_keywords = [
            "DROP",
            "DELETE",
            "UPDATE",
            "INSERT",
            "ALTER",
            "CREATE",
            "TRUNCATE",
        ]
        if any(keyword in sql_upper for keyword in dangerous_keywords):
            raise ValueError(f"SQL query contains dangerous operation: {sql}")

        # Ensure it's a SELECT query
        if not sql_upper.strip().startswith("SELECT"):
            raise ValueError("Only SELECT queries are allowed")

        return sql

    def _enhance_cube_results(self, results: Dict, context: Dict) -> Dict:
        """Enhance Cube.js results with metadata"""
        return {
            "data": results.get("data", []),
            "total_rows": len(results.get("data", [])),
            "query_info": results.get("query", {}),
            "data_source": "cube_js",
            "enhanced_metadata": {
                "available_measures": len(context.get("available_metrics", [])),
                "available_dimensions": len(context.get("available_dimensions", [])),
                "query_complexity": "semantic",
            },
        }

    def _enhance_database_results(self, results: Dict, context: Dict) -> Dict:
        """Enhance database results with metadata"""
        return {
            "data": results.get("data", []),
            "total_rows": len(results.get("data", [])),
            "columns": results.get("columns", []),
            "data_source": "database",
            "enhanced_metadata": {
                "table_count": len(context.get("table_schemas", {})),
                "query_type": "sql",
                "execution_time": results.get("execution_time", "unknown"),
            },
        }

    async def chat_completion(self, messages: List[Dict[str, str]]) -> Dict[str, Any]:
        """Handle standard chat completion without data analysis"""
        try:
            logger.info(f"üí¨ Processing chat completion with {len(messages)} messages")

            # Get the last user message
            user_message = None
            for msg in reversed(messages):
                if msg.get("role") == "user":
                    user_message = msg.get("content", "")
                    break

            if not user_message:
                return {"success": False, "error": "No user message found"}

            # Generate AI response
            ai_response = await self._get_ai_response(user_message)

            return {
                "success": True,
                "response": ai_response,
                "model_used": self._get_current_model(),
                "timestamp": datetime.now(timezone.utc).isoformat(),
            }

        except Exception as e:
            logger.error(f"‚ùå Chat completion failed: {str(e)}")
            return {"success": False, "error": str(e)}

    async def _validate_analysis_result(self, result: Dict, context: Dict) -> Dict:
        """Validate and enhance analysis results"""
        # Add validation metadata
        result["validation"] = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "data_source_count": len(context.get("cube_js_sources", []))
            + len(context.get("database_sources", [])),
            "result_quality": self._assess_result_quality(result),
            "confidence_score": self._calculate_confidence_score(result, context),
        }

        return result

    def _assess_result_quality(self, result: Dict) -> str:
        """Assess the quality of analysis results"""
        data_count = result.get("results", {}).get("total_rows", 0)

        if data_count == 0:
            return "no_data"
        elif data_count < 10:
            return "low_data"
        elif data_count < 100:
            return "moderate_data"
        else:
            return "high_data"

    def _calculate_confidence_score(self, result: Dict, context: Dict) -> float:
        """Calculate confidence score for the analysis"""
        base_score = 0.7

        # Boost score for Cube.js (semantic layer is more reliable)
        if result.get("type") == "cube_js_analysis":
            base_score += 0.2

        # Boost score for more data sources
        source_count = len(context.get("cube_js_sources", [])) + len(
            context.get("database_sources", [])
        )
        if source_count > 1:
            base_score += 0.1

        # Reduce score for low data quality
        quality = result.get("validation", {}).get("result_quality", "moderate_data")
        if quality == "no_data":
            base_score -= 0.3
        elif quality == "low_data":
            base_score -= 0.1

        return min(1.0, max(0.0, base_score))

    def _generate_table_config(self, data: List[Dict], analysis: Dict) -> Dict:
        """Generate ECharts table configuration"""
        if not data:
            return {"type": "table", "data": []}

        # Extract column names from first row
        columns = list(data[0].keys()) if data else []

        return {
            "type": "table",
            "title": "Data Table",
            "columns": columns,
            "data": data,
            "pagination": {"pageSize": 20},
        }

    async def _data_understanding_agent(
        self, query: str, data_context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Agent specialized in understanding data structure and context"""
        try:
            system_prompt = f"""You are a Data Understanding Specialist. Your role is to analyze the data context and provide insights about:

1. Data Structure: Tables, columns, relationships
2. Data Quality: Completeness, consistency, patterns
3. Data Characteristics: Types, distributions, anomalies
4. Business Context: What this data represents

Data Context:
{json.dumps(data_context, indent=2)}

User Query: {query}

Provide a structured analysis focusing on data understanding aspects."""

            response = await self.litellm_service.generate_completion(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {
                        "role": "user",
                        "content": "Analyze this data context and provide understanding insights.",
                    },
                ],
                max_tokens=800,
                temperature=0.3,
            )

            return {
                "analysis": response.get("content", ""),
                "data_structure_summary": self._extract_data_structure_summary(
                    data_context
                ),
                "quality_indicators": self._assess_data_quality(data_context),
            }

        except Exception as e:
            logger.error(f"Data understanding agent failed: {str(e)}")
            return {"analysis": "Unable to analyze data structure", "error": str(e)}

    async def _analysis_planning_agent(
        self,
        query: str,
        data_context: Dict[str, Any],
        data_understanding: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Agent specialized in planning the analysis approach"""
        try:
            system_prompt = f"""You are an Analysis Planning Specialist. Based on the user query and data understanding, create a structured analysis plan:

1. Analysis Objectives: What we need to achieve
2. Methodology: How to approach the analysis
3. Key Metrics: What to measure and track
4. Expected Outcomes: What insights we should find
5. Potential Challenges: What might be difficult

User Query: {query}
Data Understanding: {json.dumps(data_understanding, indent=2)}

Create a comprehensive analysis plan."""

            response = await self.litellm_service.generate_completion(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {
                        "role": "user",
                        "content": "Create an analysis plan for this query.",
                    },
                ],
                max_tokens=600,
                temperature=0.4,
            )

            return {
                "plan": response.get("content", ""),
                "objectives": self._extract_analysis_objectives(
                    response.get("content", "")
                ),
                "methodology": self._extract_methodology(response.get("content", "")),
            }

        except Exception as e:
            logger.error(f"Analysis planning agent failed: {str(e)}")
            return {"plan": "Unable to create analysis plan", "error": str(e)}

    async def _sql_generation_agent(
        self, query: str, data_context: Dict[str, Any], analysis_plan: Dict[str, Any]
    ) -> List[str]:
        """Agent specialized in generating SQL queries"""
        try:
            system_prompt = f"""You are a SQL Generation Specialist. Generate SQL queries based on the user query and analysis plan.

Data Context:
{json.dumps(data_context, indent=2)}

Analysis Plan:
{json.dumps(analysis_plan, indent=2)}

User Query: {query}

Generate SQL queries that:
1. Are optimized for the specific database type
2. Follow best practices for performance
3. Include proper error handling considerations
4. Are well-commented and readable

Provide the SQL queries in code blocks with explanations."""

            response = await self.litellm_service.generate_completion(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {
                        "role": "user",
                        "content": "Generate SQL queries for this analysis.",
                    },
                ],
                max_tokens=1000,
                temperature=0.2,
            )

            # Extract SQL queries from response
            sql_queries = self._extract_sql_queries(response.get("content", ""))
            return sql_queries

        except Exception as e:
            logger.error(f"SQL generation agent failed: {str(e)}")
            return []

    async def _chart_recommendation_agent(
        self, query: str, data_context: Dict[str, Any], analysis_plan: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Agent specialized in recommending chart types and configurations"""
        try:
            system_prompt = f"""You are a Chart Recommendation Specialist. Recommend the best chart types and configurations for the analysis.

Data Context:
{json.dumps(data_context, indent=2)}

Analysis Plan:
{json.dumps(analysis_plan, indent=2)}

User Query: {query}

Recommend charts that:
1. Best represent the data relationships
2. Are appropriate for the data types
3. Follow data visualization best practices
4. Include proper ECharts configuration
5. Are interactive and user-friendly

Provide specific chart recommendations with ECharts options."""

            response = await self.litellm_service.generate_completion(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": "Recommend charts for this analysis."},
                ],
                max_tokens=800,
                temperature=0.3,
            )

            # Extract chart recommendations
            chart_recommendations = self._extract_chart_recommendations(
                response.get("content", "")
            )
            return chart_recommendations

        except Exception as e:
            logger.error(f"Chart recommendation agent failed: {str(e)}")
            return []

    async def _insight_generation_agent(
        self,
        query: str,
        data_context: Dict[str, Any],
        analysis_plan: Dict[str, Any],
        sql_queries: List[str],
    ) -> Dict[str, Any]:
        """Agent specialized in generating actionable insights"""
        try:
            system_prompt = f"""You are an Insight Generation Specialist. Generate actionable business insights based on the analysis.

Data Context:
{json.dumps(data_context, indent=2)}

Analysis Plan:
{json.dumps(analysis_plan, indent=2)}

SQL Queries: {json.dumps(sql_queries, indent=2)}

User Query: {query}

Generate insights that:
1. Are actionable and specific
2. Include business implications
3. Provide recommendations
4. Identify trends and patterns
5. Highlight risks and opportunities

Focus on practical business value."""

            response = await self.litellm_service.generate_completion(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {
                        "role": "user",
                        "content": "Generate business insights for this analysis.",
                    },
                ],
                max_tokens=1000,
                temperature=0.4,
            )

            return {
                "insights": response.get("content", ""),
                "key_findings": self._extract_key_findings(response.get("content", "")),
                "recommendations": self._extract_recommendations(
                    response.get("content", "")
                ),
            }

        except Exception as e:
            logger.error(f"Insight generation agent failed: {str(e)}")
            return {"insights": "Unable to generate insights", "error": str(e)}

    async def _business_context_agent(
        self, query: str, data_context: Dict[str, Any], insights: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Agent specialized in providing business context and implications"""
        try:
            system_prompt = f"""You are a Business Context Specialist. Provide business context and implications for the analysis results.

Data Context:
{json.dumps(data_context, indent=2)}

Generated Insights:
{json.dumps(insights, indent=2)}

User Query: {query}

Provide business context that:
1. Explains the business significance
2. Identifies stakeholders affected
3. Suggests next steps
4. Highlights competitive implications
5. Recommends strategic actions

Focus on business value and strategic impact."""

            response = await self.litellm_service.generate_completion(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {
                        "role": "user",
                        "content": "Provide business context for these insights.",
                    },
                ],
                max_tokens=800,
                temperature=0.4,
            )

            return {
                "business_context": response.get("content", ""),
                "stakeholders": self._extract_stakeholders(response.get("content", "")),
                "strategic_actions": self._extract_strategic_actions(
                    response.get("content", "")
                ),
            }

        except Exception as e:
            logger.error(f"Business context agent failed: {str(e)}")
            return {
                "business_context": "Unable to provide business context",
                "error": str(e),
            }

    async def _synthesize_response(
        self,
        query: str,
        data_context: Dict[str, Any],
        analysis_plan: Dict[str, Any],
        sql_queries: List[str],
        chart_recommendations: List[Dict[str, Any]],
        insights: Dict[str, Any],
        business_context: Dict[str, Any],
    ) -> str:
        """Synthesize all agent outputs into a coherent response"""
        try:
            system_prompt = f"""You are a Response Synthesis Specialist. Combine all analysis components into a coherent, professional response.

User Query: {query}

Analysis Components:
- Analysis Plan: {json.dumps(analysis_plan, indent=2)}
- SQL Queries: {json.dumps(sql_queries, indent=2)}
- Chart Recommendations: {json.dumps(chart_recommendations, indent=2)}
- Insights: {json.dumps(insights, indent=2)}
- Business Context: {json.dumps(business_context, indent=2)}

Create a comprehensive response that:
1. Directly answers the user's query
2. Integrates all analysis components seamlessly
3. Is well-structured and easy to follow
4. Provides actionable recommendations
5. Maintains professional tone and clarity

Structure the response logically and ensure all components are properly integrated."""

            response = await self.litellm_service.generate_completion(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {
                        "role": "user",
                        "content": "Synthesize all analysis components into a coherent response.",
                    },
                ],
                max_tokens=1500,
                temperature=0.3,
            )

            return response.get("content", "Unable to synthesize response")

        except Exception as e:
            logger.error(f"Response synthesis failed: {str(e)}")
            return f"Analysis completed but synthesis failed: {str(e)}"

    async def _get_model_config(
        self, model_id: Optional[str] = None
    ) -> Optional[Dict[str, Any]]:
        """Get model configuration for AI orchestration"""
        try:
            # Use the existing LiteLLM service to get model config
            if model_id:
                return self.litellm_service.get_model_info(model_id)
            else:
                return self.litellm_service.get_model_info()
        except Exception as e:
            logger.error(f"‚ùå Failed to get model config: {str(e)}")
            return None

    async def _fallback_analysis(
        self, query: str, context: Dict, error: str
    ) -> Dict[str, Any]:
        """Fallback analysis when primary methods fail"""
        try:
            # Generate a helpful fallback response
            fallback_prompt = f"""
            The user asked: "{query}"
            
            However, the analysis failed with error: {error}
            
            Available data sources: {self._build_context_summary(context)}
            
            Please provide:
            1. An explanation of what went wrong
            2. Suggestions for how to fix the issue
            3. Alternative approaches to get the desired information
            4. Any available data that might be helpful
            """

            fallback_response = await self._get_ai_response(fallback_prompt)

            return {
                "type": "fallback_analysis",
                "query": query,
                "error": error,
                "fallback_suggestions": fallback_response,
                "available_data_summary": self._build_context_summary(context),
                "visualization_config": {"type": "text", "content": fallback_response},
            }

        except Exception as e:
            logger.error(f"Fallback analysis also failed: {e}")
            return {
                "type": "error",
                "query": query,
                "error": f"Primary analysis failed: {error}. Fallback also failed: {str(e)}",
                "suggestion": "Please check your data source connections and try again.",
            }

    # Helper methods for extracting information
    def _extract_data_structure_summary(self, data_context: Dict[str, Any]) -> str:
        """Extract summary of data structure"""
        if data_context.get("type") == "database" and data_context.get("schema"):
            tables = data_context["schema"].get("tables", [])
            return f"{len(tables)} tables with {sum(t.get('rowCount', 0) for t in tables)} total rows"
        elif data_context.get("type") == "file":
            return f"File with {data_context.get('file_info', {}).get('row_count', 0)} rows"
        elif data_context.get("type") == "cube":
            cubes = data_context.get("cube_schema", {}).get("cubes", [])
            return f"{len(cubes)} cubes with dimensions and measures"
        return "Unknown data structure"

    def _assess_data_quality(self, data_context: Dict[str, Any]) -> Dict[str, Any]:
        """Assess data quality indicators"""
        quality = {
            "completeness": "unknown",
            "consistency": "unknown",
            "freshness": "unknown",
        }

        # Add quality assessment logic based on data context
        if data_context.get("type") == "database":
            quality["completeness"] = "assessable"
            quality["consistency"] = "assessable"

        return quality

    def _extract_analysis_objectives(self, content: str) -> List[str]:
        """Extract analysis objectives from content"""
        # Simple extraction - look for numbered or bulleted objectives
        objectives = []
        lines = content.split("\n")
        for line in lines:
            if line.strip().startswith(("1.", "2.", "3.", "4.", "5.", "-", "‚Ä¢")):
                objectives.append(line.strip())
        return objectives[:5]  # Limit to 5 objectives

    def _extract_methodology(self, content: str) -> str:
        """Extract methodology from content"""
        # Look for methodology section
        if "methodology" in content.lower():
            start = content.lower().find("methodology")
            end = content.find("\n", start + 10)
            if end > start:
                return content[start:end].strip()
        return "Methodology not specified"

    def _extract_sql_queries(self, content: str) -> List[str]:
        """Extract SQL queries from content"""
        sql_pattern = r"```sql\s*([\s\S]*?)```"
        matches = re.findall(sql_pattern, content, re.IGNORECASE)
        return [query.strip() for query in matches if query.strip()]

    def _extract_chart_recommendations(self, content: str) -> List[Dict[str, Any]]:
        """Extract chart recommendations from content"""
        # Look for chart recommendations in the content
        recommendations = []
        if "chart" in content.lower() or "visualization" in content.lower():
            recommendations.append(
                {
                    "type": "auto-detected",
                    "description": "Chart recommendation found in analysis",
                    "content": content,
                }
            )
        return recommendations

    def _extract_key_findings(self, content: str) -> List[str]:
        """Extract key findings from insights"""
        findings = []
        lines = content.split("\n")
        for line in lines:
            if any(
                keyword in line.lower()
                for keyword in ["finding", "discovery", "pattern", "trend"]
            ):
                findings.append(line.strip())
        return findings[:3]  # Limit to 3 findings

    def _extract_recommendations(self, content: str) -> List[str]:
        """Extract recommendations from insights"""
        recommendations = []
        lines = content.split("\n")
        for line in lines:
            if any(
                keyword in line.lower()
                for keyword in ["recommend", "suggest", "should", "action"]
            ):
                recommendations.append(line.strip())
        return recommendations[:3]  # Limit to 3 recommendations

    def _extract_stakeholders(self, content: str) -> List[str]:
        """Extract stakeholders from business context"""
        stakeholders = []
        lines = content.split("\n")
        for line in lines:
            if (
                "stakeholder" in line.lower()
                or "team" in line.lower()
                or "department" in line.lower()
            ):
                stakeholders.append(line.strip())
        return stakeholders[:3]

    def _extract_strategic_actions(self, content: str) -> List[str]:
        """Extract strategic actions from business context"""
        actions = []
        lines = content.split("\n")
        for line in lines:
            if any(
                keyword in line.lower()
                for keyword in ["action", "strategy", "plan", "initiative"]
            ):
                actions.append(line.strip())
        return actions[:3]

    def _build_cube_schema_summary(self, cubes: List[Dict]) -> str:
        """Build human-readable summary of Cube.js schema"""
        if not cubes:
            return "No cubes available"

        summaries = []
        for cube in cubes:
            cube_name = cube.get("name", "Unknown")
            measures_count = len(cube.get("measures", []))
            dimensions_count = len(cube.get("dimensions", []))
            summaries.append(
                f"{cube_name}: {measures_count} measures, {dimensions_count} dimensions"
            )

        return " | ".join(summaries)

    def _generate_cube_query_templates(
        self, measures: List[Dict], dimensions: List[Dict]
    ) -> List[Dict]:
        """Generate example query templates for AI to use"""
        templates = []

        # Time series analysis template
        time_dimensions = [
            d
            for d in dimensions
            if "time" in d.get("type", "").lower()
            or "date" in d.get("name", "").lower()
        ]
        if time_dimensions and measures:
            templates.append(
                {
                    "type": "time_series",
                    "description": "Analyze trends over time",
                    "measures": [m["name"] for m in measures[:2]],
                    "dimensions": [d["name"] for d in time_dimensions[:1]],
                    "example": f"Show {measures[0]['name']} over time by {time_dimensions[0]['name']}",
                }
            )

        # Aggregation template
        if measures:
            templates.append(
                {
                    "type": "aggregation",
                    "description": "Calculate totals and averages",
                    "measures": [m["name"] for m in measures[:3]],
                    "dimensions": [],
                    "example": f"Calculate total {measures[0]['name']} and average {measures[1]['name'] if len(measures) > 1 else measures[0]['name']}",
                }
            )

        # Dimension analysis template
        if dimensions and measures:
            templates.append(
                {
                    "type": "dimension_analysis",
                    "description": "Break down by categories",
                    "measures": [m["name"] for m in measures[:1]],
                    "dimensions": [d["name"] for d in dimensions[:2]],
                    "example": f"Show {measures[0]['name']} by {dimensions[0]['name']} and {dimensions[1]['name'] if len(dimensions) > 1 else dimensions[0]['name']}",
                }
            )

        return templates

    def _determine_recommended_strategy(self, context: Dict) -> str:
        """Determine the recommended analysis strategy based on available sources"""
        if context["cube_js_sources"]:
            return "cube_js_semantic"  # Best for business users
        elif context["database_sources"]:
            return "database_sql"  # Good for technical users
        elif context["file_sources"]:
            return "file_analysis"  # Basic analysis
        else:
            return "general_guidance"

    def _get_supported_chart_types(self, context: Dict) -> List[str]:
        """Get supported chart types based on available data sources"""
        chart_types = []

        if context["cube_js_sources"]:
            chart_types.extend(
                ["line", "bar", "pie", "scatter", "area", "heatmap", "table"]
            )
        if context["database_sources"]:
            chart_types.extend(
                ["line", "bar", "pie", "scatter", "area", "table", "dashboard"]
            )
        if context["file_sources"]:
            chart_types.extend(["bar", "pie", "scatter", "histogram", "table"])

        return list(set(chart_types))  # Remove duplicates

    def _get_query_capabilities(self, context: Dict) -> Dict:
        """Get query capabilities for each data source type"""
        capabilities = {}

        if context["cube_js_sources"]:
            capabilities["cube_js"] = {
                "query_format": "semantic",
                "example": "measures: ['Sales.amount'], dimensions: ['Category.name'], timeDimensions: [{dimension: 'Sales.orderDate', granularity: 'month'}]",
                "advantages": [
                    "Business-friendly",
                    "Pre-aggregated",
                    "Semantic modeling",
                    "Performance optimized",
                ],
            }

        if context["database_sources"]:
            capabilities["database"] = {
                "query_format": "sql",
                "example": "SELECT category, SUM(amount) FROM sales GROUP BY category ORDER BY SUM(amount) DESC",
                "advantages": [
                    "Full SQL power",
                    "Complex joins",
                    "Custom logic",
                    "Real-time data",
                ],
            }

        if context["file_sources"]:
            capabilities["file"] = {
                "query_format": "column_based",
                "example": "Analyze 'sales_amount' column grouped by 'category' column",
                "advantages": [
                    "Simple structure",
                    "Quick analysis",
                    "No setup required",
                    "Direct access",
                ],
            }

        return capabilities

    def _build_database_schema_summary(self, schema_data: Dict) -> str:
        """Build human-readable database schema summary"""
        tables = schema_data.get("tables", {})
        schemas = schema_data.get("schemas", [])

        summary = (
            f"Database with {len(tables)} tables across {len(schemas)} schemas\n\n"
        )

        for table_name, table_info in tables.items():
            columns = table_info.get("columns", [])
            summary += f"Table: {table_name}\n"
            summary += f"  Columns: {len(columns)}\n"

            # Group columns by type
            numeric_cols = [
                col
                for col in columns
                if col.get("data_type") in ["integer", "numeric", "decimal", "float"]
            ]
            text_cols = [
                col
                for col in columns
                if col.get("data_type") in ["varchar", "text", "string"]
            ]
            date_cols = [
                col
                for col in columns
                if col.get("data_type") in ["timestamp", "date", "time"]
            ]

            if numeric_cols:
                summary += f"  Numeric columns: {len(numeric_cols)} (e.g., {', '.join([col['name'] for col in numeric_cols[:3]])})\n"
            if text_cols:
                summary += f"  Text columns: {len(text_cols)} (e.g., {', '.join([col['name'] for col in text_cols[:3]])})\n"
            if date_cols:
                summary += f"  Date columns: {len(date_cols)} (e.g., {', '.join([col['name'] for col in date_cols[:3]])})\n"

            summary += "\n"

        return summary

    def _generate_file_query_templates(self, schema_data: Dict) -> List[Dict]:
        """Generate analysis templates for file data"""
        templates = []
        columns = schema_data.get("columns", [])

        numeric_cols = [
            col for col in columns if col.get("type") in ["number", "integer", "float"]
        ]
        text_cols = [col for col in columns if col.get("type") == "string"]

        if numeric_cols and text_cols:
            templates.append(
                {
                    "type": "aggregation",
                    "template": f"Group by {text_cols[0]['name']} and calculate statistics for {numeric_cols[0]['name']}",
                    "description": f"Analyze {numeric_cols[0]['name']} grouped by {text_cols[0]['name']}",
                }
            )

        if len(numeric_cols) >= 2:
            templates.append(
                {
                    "type": "correlation",
                    "template": f"Analyze correlation between {numeric_cols[0]['name']} and {numeric_cols[1]['name']}",
                    "description": f"Find relationship between {numeric_cols[0]['name']} and {numeric_cols[1]['name']}",
                }
            )

        return templates

    def _build_file_schema_summary(self, source: Dict, schema_data: Dict) -> str:
        """Build human-readable file schema summary"""
        columns = schema_data.get("columns", [])
        row_count = source.get("row_count", 0)

        summary = f"File: {source['name']}\n"
        summary += f"Type: {source.get('format', 'unknown')}\n"
        summary += f"Size: {source.get('size', 'unknown')} bytes\n"
        summary += f"Rows: {row_count}\n"
        summary += f"Columns: {len(columns)}\n\n"

        # Group columns by type
        numeric_cols = [
            col for col in columns if col.get("type") in ["number", "integer", "float"]
        ]
        text_cols = [col for col in columns if col.get("type") == "string"]
        date_cols = [
            col for col in columns if col.get("type") in ["date", "time", "datetime"]
        ]

        if numeric_cols:
            summary += f"Numeric columns ({len(numeric_cols)}):\n"
            for col in numeric_cols[:5]:  # Show first 5
                stats = col.get("statistics", {})
                summary += f"  - {col['name']}: {col['type']}"
                if stats:
                    summary += f" (min: {stats.get('min', 'N/A')}, max: {stats.get('max', 'N/A')}, unique: {stats.get('unique_count', 'N/A')})"
                summary += "\n"
            summary += "\n"

        if text_cols:
            summary += f"Text columns ({len(text_cols)}):\n"
            for col in text_cols[:5]:  # Show first 5
                stats = col.get("statistics", {})
                summary += f"  - {col['name']}: {col['type']}"
                if stats:
                    summary += f" (unique: {stats.get('unique_count', 'N/A')}, max length: {stats.get('max_length', 'N/A')})"
                summary += "\n"
            summary += "\n"

        if date_cols:
            summary += f"Date columns ({len(date_cols)}):\n"
            for col in date_cols:
                summary += f"  - {col['name']}: {col['type']}\n"
            summary += "\n"

        return summary

    def _has_numeric_columns(self, schema_data: Dict) -> bool:
        """Check if file has numeric columns"""
        columns = schema_data.get("columns", [])
        return any(col.get("type") in ["number", "integer", "float"] for col in columns)

    def _has_time_columns_file(self, schema_data: Dict) -> bool:
        """Check if file has time-related columns"""
        columns = schema_data.get("columns", [])
        for col in columns:
            col_type = col.get("type", "").lower()
            col_name = col.get("name", "").lower()
            if any(
                time_word in col_type for time_word in ["date", "time", "datetime"]
            ) or any(
                time_word in col_name
                for time_word in ["date", "time", "created", "updated"]
            ):
                return True
        return False

    def _get_file_data_types(self, schema_data: Dict) -> Dict:
        """Get data type distribution for file"""
        columns = schema_data.get("columns", [])
        type_counts = {}

        for col in columns:
            col_type = col.get("type", "unknown")
            type_counts[col_type] = type_counts.get(col_type, 0) + 1

        return type_counts
