"""
Cube.js Connector Integration Service
Leverages Cube.js pre-built database connectors through API
"""

import logging
import asyncio
import aiohttp
from typing import Dict, Any
import os

logger = logging.getLogger(__name__)


class CubeConnectorService:
    """Service for integrating with Cube.js pre-built database connectors"""

    def __init__(self):
        self.cube_api_url = os.getenv(
            "CUBE_API_URL", "http://localhost:4000/cubejs-api/v1"
        )
        self.cube_api_secret = os.getenv("CUBE_API_SECRET", "dev-cube-secret-key")

        # Cube.js supported database types
        self.supported_databases = {
            "postgresql": {
                "driver": "@cubejs-backend/postgres-driver",
                "port": 5432,
                "ssl_support": True,
            },
            "mysql": {
                "driver": "@cubejs-backend/mysql-driver",
                "port": 3306,
                "ssl_support": True,
            },
            "sqlserver": {
                "driver": "@cubejs-backend/mssql-driver",
                "port": 1433,
                "ssl_support": True,
            },
            "snowflake": {
                "driver": "@cubejs-backend/snowflake-driver",
                "port": 443,
                "ssl_support": True,
            },
            "bigquery": {
                "driver": "@cubejs-backend/bigquery-driver",
                "port": None,
                "ssl_support": True,
            },
            "redshift": {
                "driver": "@cubejs-backend/redshift-driver",
                "port": 5439,
                "ssl_support": True,
            },
            "clickhouse": {
                "driver": "@cubejs-backend/clickhouse-driver",
                "port": 8123,
                "ssl_support": True,
            },
        }

    async def create_cube_data_source(
        self, connection_config: Dict[str, Any], tenant_id: str = "default"
    ) -> Dict[str, Any]:
        """Create data source using Cube.js connectors"""
        try:
            logger.info(
                f"üîå Creating Cube.js data source: {connection_config.get('type')}"
            )

            db_type = connection_config.get("type")
            if db_type not in self.supported_databases:
                raise ValueError(
                    f"Unsupported database type: {db_type}. Supported: {list(self.supported_databases.keys())}"
                )

            # Validate connection configuration
            self._validate_connection_config(connection_config, db_type)

            # Generate Cube.js environment variables for this connection
            cube_env = self._generate_cube_env_config(connection_config, tenant_id)

            # Test connection through Cube.js
            connection_test = await self._test_cube_connection(cube_env, tenant_id)

            if not connection_test["success"]:
                raise Exception(
                    f"Cube.js connection test failed: {connection_test['error']}"
                )

            # Get schema information from Cube.js
            schema_info = await self._get_cube_schema_info(tenant_id)

            return {
                "success": True,
                "data_source": {
                    "id": f"cube_{db_type}_{tenant_id}_{int(asyncio.get_event_loop().time())}",
                    "name": connection_config.get("name", f"{db_type}_connection"),
                    "type": "database",
                    "db_type": db_type,
                    "driver": self.supported_databases[db_type]["driver"],
                    "tenant_id": tenant_id,
                    "cube_env_config": cube_env,
                    "schema_info": schema_info,
                    "status": "connected",
                },
            }

        except Exception as error:
            logger.error(f"‚ùå Cube.js data source creation failed: {str(error)}")
            return {"success": False, "error": str(error)}

    async def query_cube_data_source(
        self,
        data_source_id: str,
        cube_query: Dict[str, Any],
        tenant_id: str = "default",
    ) -> Dict[str, Any]:
        """Query data through Cube.js connector"""
        try:
            logger.info(f"üîç Querying Cube.js data source: {data_source_id}")

            # Execute query through Cube.js API
            async with aiohttp.ClientSession() as session:
                headers = {
                    "Authorization": f"Bearer {self.cube_api_secret}",
                    "Content-Type": "application/json",
                    "x-tenant-id": tenant_id,
                }

                async with session.post(
                    f"{self.cube_api_url}/load",
                    json={"query": cube_query},
                    headers=headers,
                ) as response:
                    if response.status == 200:
                        result = await response.json()

                        return {
                            "success": True,
                            "data": result.get("data", []),
                            "query": cube_query,
                            "annotation": result.get("annotation", {}),
                            "total_rows": len(result.get("data", [])),
                            "execution_time": result.get("executionTime"),
                        }
                    else:
                        error_text = await response.text()
                        raise Exception(
                            f"Cube.js query failed: {response.status} - {error_text}"
                        )

        except Exception as error:
            logger.error(f"‚ùå Cube.js query failed: {str(error)}")
            return {"success": False, "error": str(error)}

    async def get_cube_schema(self, tenant_id: str = "default") -> Dict[str, Any]:
        """Get schema information from Cube.js"""
        try:
            async with aiohttp.ClientSession() as session:
                headers = {
                    "Authorization": f"Bearer {self.cube_api_secret}",
                    "x-tenant-id": tenant_id,
                }

                async with session.get(
                    f"{self.cube_api_url}/meta", headers=headers
                ) as response:
                    if response.status == 200:
                        schema = await response.json()
                        return {
                            "success": True,
                            "schema": schema,
                            "cubes": schema.get("cubes", []),
                            "tenant_id": tenant_id,
                        }
                    else:
                        error_text = await response.text()
                        raise Exception(
                            f"Schema fetch failed: {response.status} - {error_text}"
                        )

        except Exception as error:
            logger.error(f"‚ùå Cube.js schema fetch failed: {str(error)}")
            return {"success": False, "error": str(error)}

    async def test_connection(
        self, connection_config: Dict[str, Any], tenant_id: str = "default"
    ) -> Dict[str, Any]:
        """Test database connection through Cube.js"""
        try:
            logger.info(
                f"üß™ Testing Cube.js connection: {connection_config.get('type')}"
            )

            db_type = connection_config.get("type")
            if db_type not in self.supported_databases:
                return {
                    "success": False,
                    "error": f"Unsupported database type: {db_type}",
                }

            # Validate connection configuration
            try:
                self._validate_connection_config(connection_config, db_type)
            except ValueError as e:
                return {"success": False, "error": str(e)}

            # Generate Cube.js environment variables for this connection
            cube_env = self._generate_cube_env_config(connection_config, tenant_id)

            # Test connection through Cube.js
            connection_test = await self._test_cube_connection(cube_env, tenant_id)

            if connection_test["success"]:
                return {
                    "success": True,
                    "connection_info": {
                        "type": db_type,
                        "host": connection_config.get("host"),
                        "port": connection_config.get("port"),
                        "database": connection_config.get("database"),
                        "status": "connected",
                        "driver": self.supported_databases[db_type]["driver"],
                    },
                }
            else:
                return {
                    "success": False,
                    "error": connection_test.get("error", "Connection test failed"),
                }

        except Exception as e:
            logger.error(f"‚ùå Connection test failed: {str(e)}")
            return {"success": False, "error": f"Connection test failed: {str(e)}"}

    def get_supported_databases(self) -> Dict[str, Any]:
        """Get list of supported database types"""
        return {
            "success": True,
            "supported_databases": [
                {
                    "type": db_type,
                    "name": db_type.title(),
                    "driver": config["driver"],
                    "default_port": config["port"],
                    "ssl_support": config["ssl_support"],
                }
                for db_type, config in self.supported_databases.items()
            ],
        }

    # Private helper methods
    def _validate_connection_config(self, config: Dict[str, Any], db_type: str):
        """Validate connection configuration"""
        required_fields = ["host", "database", "username", "password"]

        for field in required_fields:
            if not config.get(field):
                raise ValueError(f"Missing required field: {field}")

        # Database-specific validation
        if db_type == "bigquery":
            if not config.get("project_id"):
                raise ValueError("BigQuery requires project_id")

        if db_type == "snowflake":
            if not config.get("account"):
                raise ValueError("Snowflake requires account")

    def _generate_cube_env_config(
        self, connection_config: Dict[str, Any], tenant_id: str
    ) -> Dict[str, str]:
        """Generate Cube.js environment configuration"""
        db_type = connection_config["type"]

        # Base configuration
        cube_env = {
            "CUBEJS_DB_TYPE": db_type,
            "CUBEJS_DB_HOST": connection_config["host"],
            "CUBEJS_DB_NAME": connection_config["database"],
            "CUBEJS_DB_USER": connection_config["username"],
            "CUBEJS_DB_PASS": connection_config["password"],
            "CUBEJS_DB_PORT": str(
                connection_config.get("port", self.supported_databases[db_type]["port"])
            ),
        }

        # Add SSL configuration if supported
        if connection_config.get("ssl", False):
            cube_env["CUBEJS_DB_SSL"] = "true"

        # Database-specific configuration
        if db_type == "postgresql":
            cube_env["CUBEJS_DB_SCHEMA"] = connection_config.get("schema", "public")
        elif db_type == "bigquery":
            cube_env["CUBEJS_DB_BQ_PROJECT_ID"] = connection_config["project_id"]
            if connection_config.get("key_file"):
                cube_env["CUBEJS_DB_BQ_KEY_FILE"] = connection_config["key_file"]
        elif db_type == "snowflake":
            cube_env["CUBEJS_DB_SNOWFLAKE_ACCOUNT"] = connection_config["account"]
            cube_env["CUBEJS_DB_SNOWFLAKE_REGION"] = connection_config.get(
                "region", "us-west-2"
            )

        # Add tenant isolation
        cube_env["CUBEJS_DB_SCHEMA"] = (
            f"{connection_config.get('schema', 'public')}_{tenant_id}"
        )

        return cube_env

    async def _test_cube_connection(
        self, cube_env: Dict[str, str], tenant_id: str
    ) -> Dict[str, Any]:
        """Test connection through Cube.js"""
        try:
            # This would typically involve calling a Cube.js connection test endpoint
            # For now, we simulate a successful connection test
            logger.info(f"üß™ Testing Cube.js connection for tenant: {tenant_id}")

            # In production, this would make an actual test query to Cube.js
            # await self._make_test_query(cube_env, tenant_id)

            return {
                "success": True,
                "message": "Connection test successful",
                "tenant_id": tenant_id,
            }

        except Exception as error:
            return {"success": False, "error": str(error), "tenant_id": tenant_id}

    async def _get_cube_schema_info(self, tenant_id: str) -> Dict[str, Any]:
        """Get schema information from Cube.js"""
        try:
            schema_result = await self.get_cube_schema(tenant_id)
            if schema_result["success"]:
                return schema_result["schema"]
            else:
                return {}
        except:
            return {}

    async def get_database_schema(self, connection_config: Dict[str, Any]) -> Dict[str, Any]:
        """Get database schema information"""
        try:
            logger.info(f"üîç Getting database schema for: {connection_config.get('type')}")
            
            # Use the real cube integration service to get schema
            from app.modules.data.services.real_cube_integration_service import RealCubeIntegrationService
            real_cube_service = RealCubeIntegrationService()
            
            # Create a temporary connection to get schema
            # First test the connection
            test_result = await self.test_connection(connection_config)
            if not test_result.get('success'):
                return {
                    'success': False,
                    'error': f"Connection test failed: {test_result.get('error')}"
                }
            
            # Get schema using SQLAlchemy directly
            db_type = connection_config.get('type', '').lower()
            if db_type not in self.supported_databases:
                return {
                    'success': False,
                    'error': f'Unsupported database type: {db_type}'
                }
            
            # Use real_cube_integration_service to get schema
            # We need to create a connection and get schema
            try:
                from sqlalchemy import create_engine, inspect
                from sqlalchemy.engine import URL
                
                # Build connection URL
                host = connection_config.get('host', 'localhost')
                port = connection_config.get('port', self.supported_databases[db_type].get('port', 5432))
                database = connection_config.get('database') or connection_config.get('db')
                username = connection_config.get('username') or connection_config.get('user')
                password = connection_config.get('password') or connection_config.get('pass')
                
                if not database:
                    return {
                        'success': False,
                        'error': 'Database name is required'
                    }
                
                # Create SQLAlchemy URL
                if db_type == 'postgresql':
                    url = URL.create(
                        'postgresql',
                        username=username,
                        password=password,
                        host=host,
                        port=port,
                        database=database
                    )
                elif db_type == 'mysql':
                    url = URL.create(
                        'mysql+pymysql',
                        username=username,
                        password=password,
                        host=host,
                        port=port,
                        database=database
                    )
                elif db_type == 'clickhouse':
                    # ClickHouse uses HTTP interface - use HTTP API instead of SQLAlchemy
                    try:
                        import aiohttp
                        http_url = f"http://{host}:{port}"
                        query = f"SELECT name, engine FROM system.tables WHERE database = '{database}' FORMAT JSON"
                        
                        async with aiohttp.ClientSession() as session:
                            auth = aiohttp.BasicAuth(username, password)
                            async with session.post(f"{http_url}/", data=query, auth=auth) as resp:
                                if resp.status == 200:
                                    data = await resp.json()
                                    tables = []
                                    schemas = set()
                                    
                                    for table in data.get('data', []):
                                        table_name = table.get('name')
                                        
                                        # Skip internal ClickHouse tables (materialized view inner tables)
                                        if table_name.startswith('.inner_id.') or table_name.startswith('.'):
                                            continue
                                        
                                        # Get columns for each table
                                        cols_query = f"DESCRIBE TABLE {database}.{table_name} FORMAT JSON"
                                        async with session.post(f"{http_url}/", data=cols_query, auth=auth) as cols_resp:
                                            if cols_resp.status == 200:
                                                cols_data = await cols_resp.json()
                                                columns = []
                                                for col in cols_data.get('data', []):
                                                    columns.append({
                                                        'name': col.get('name', ''),
                                                        'type': str(col.get('type', '')),
                                                        'nullable': col.get('default_kind') != 'DEFAULT'
                                                    })
                                                
                                                # Get row count for table
                                                count_query = f"SELECT count() as row_count FROM {database}.{table_name} FORMAT JSON"
                                                row_count = 0
                                                try:
                                                    async with session.post(f"{http_url}/", data=count_query, auth=auth) as count_resp:
                                                        if count_resp.status == 200:
                                                            count_data = await count_resp.json()
                                                            if count_data.get('data') and len(count_data['data']) > 0:
                                                                # ClickHouse returns row_count as string, convert to int
                                                                row_count_val = count_data['data'][0].get('row_count', 0)
                                                                row_count = int(row_count_val) if row_count_val is not None else 0
                                                except Exception as count_error:
                                                    logger.debug(f"Row count fetch failed for {table_name}: {count_error}")
                                                    pass  # Row count is optional
                                                
                                                tables.append({
                                                    'schema': database,
                                                    'name': table_name,
                                                    'columns': columns,
                                                    'rowCount': int(row_count)  # Ensure it's an int
                                                })
                                                schemas.add(database)
                                    
                                    # Calculate total_rows, ensuring all values are ints
                                    total_rows = sum(int(t.get('rowCount', 0)) for t in tables)
                                    
                                    return {
                                        'success': True,
                                        'tables': tables,
                                        'schemas': list(schemas),
                                        'total_rows': total_rows
                                    }
                                else:
                                    error_text = await resp.text()
                                    return {
                                        'success': False,
                                        'error': f'ClickHouse HTTP error {resp.status}: {error_text}'
                                    }
                    except ImportError:
                        logger.error("aiohttp not available for ClickHouse schema fetching")
                        return {
                            'success': False,
                            'error': 'aiohttp package required for ClickHouse schema fetching'
                        }
                    except Exception as clickhouse_error:
                        logger.error(f"‚ùå ClickHouse schema fetch failed: {str(clickhouse_error)}")
                        return {
                            'success': False,
                            'error': f"ClickHouse schema retrieval failed: {str(clickhouse_error)}"
                        }
                else:
                    # For other database types (postgresql, mysql, sqlserver, etc.), use SQLAlchemy
                    if db_type not in ['postgresql', 'mysql', 'sqlserver', 'redshift']:
                        return {
                            'success': False,
                            'error': f'Schema retrieval not yet implemented for {db_type}'
                        }
                    
                    # Create engine and inspect schema
                    try:
                        engine = create_engine(str(url), pool_pre_ping=True, echo=False)
                        inspector = inspect(engine)
                        
                        tables = []
                        schemas_list = inspector.get_schema_names()
                        
                        # Filter out system schemas based on database type
                        system_schemas = {
                            'postgresql': ['information_schema', 'pg_catalog', 'pg_toast'],
                            'mysql': ['information_schema', 'performance_schema', 'mysql', 'sys'],
                            'sqlserver': ['information_schema', 'sys'],
                            'redshift': ['information_schema', 'pg_catalog', 'pg_toast']
                        }
                        excluded_schemas = system_schemas.get(db_type, ['information_schema', 'sys'])
                        
                        for schema_name in schemas_list:
                            if schema_name.lower() not in [s.lower() for s in excluded_schemas]:
                                try:
                                    table_names = inspector.get_table_names(schema=schema_name)
                                    for table_name in table_names:
                                        columns = []
                                        try:
                                            for col in inspector.get_columns(table_name, schema=schema_name):
                                                columns.append({
                                                    'name': col['name'],
                                                    'type': str(col['type']),
                                                    'nullable': col.get('nullable', True)
                                                })
                                        except Exception as col_error:
                                            logger.warning(f"Failed to get columns for {schema_name}.{table_name}: {col_error}")
                                            columns = []
                                        
                                        tables.append({
                                            'schema': schema_name,
                                            'name': table_name,
                                            'columns': columns,
                                            'rowCount': 0  # Would need to query each table
                                        })
                                except Exception as table_error:
                                    logger.warning(f"Failed to get tables for schema {schema_name}: {table_error}")
                                    continue
                        
                        engine.dispose()
                        
                        return {
                            'success': True,
                            'tables': tables,
                            'schemas': list(set(s['schema'] for s in tables)),
                            'total_rows': 0  # Would need to query each table
                        }
                    except Exception as sqlalchemy_error:
                        logger.error(f"‚ùå SQLAlchemy schema fetch failed for {db_type}: {str(sqlalchemy_error)}")
                        return {
                            'success': False,
                            'error': f"Schema retrieval failed: {str(sqlalchemy_error)}"
                        }
            except Exception as schema_error:
                logger.error(f"‚ùå Schema retrieval failed: {str(schema_error)}")
                return {
                    'success': False,
                    'error': f"Schema retrieval failed: {str(schema_error)}"
                }
        except Exception as e:
            logger.error(f"‚ùå Failed to get database schema: {str(e)}")
            return {
                'success': False,
                'error': str(e)
            }

    async def _make_test_query(self, cube_env: Dict[str, str], tenant_id: str):
        """Make a test query to validate connection"""
        # This would make an actual test query to the database through Cube.js
        # Implementation would depend on the specific Cube.js setup
