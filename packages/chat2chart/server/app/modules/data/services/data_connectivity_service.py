"""
Universal Data Connectivity Service
Handles file uploads, database connections, and data source management
"""

import logging
import os
import pandas as pd
import json
from typing import Dict, List, Optional, Any
from datetime import datetime
from pathlib import Path
from .cube_connector_service import CubeConnectorService
from app.modules.data.services.ai_schema_service import AISchemaService

logger = logging.getLogger(__name__)


class DataConnectivityService:
    """Service for handling data connectivity and file uploads"""

    def __init__(self):
        self.upload_dir = "./uploads"
        self.max_file_size = 50 * 1024 * 1024  # 50MB
        self.supported_formats = [
            "csv",
            "xlsx",
            "xls",
            "json",
            "tsv",
            "parquet",
            "parq",
            "snappy",
        ]

        # Enhanced file processing options
        self.file_processing_configs = {
            "csv": {
                "delimiters": [",", ";", "\t", "|", " "],
                "encodings": ["utf-8", "latin-1", "cp1252"],
                "auto_detect": True,
            },
            "tsv": {
                "delimiters": ["\t"],
                "encodings": ["utf-8", "latin-1"],
                "auto_detect": False,
            },
            "xlsx": {"sheet_selection": "auto", "header_row": 0, "skip_rows": 0},
            "xls": {"sheet_selection": "auto", "header_row": 0, "skip_rows": 0},
            "json": {"flatten_nested": True, "max_nesting_level": 3},
            "parquet": {"columns": "auto", "partitions": "auto"},
        }

        # Ensure upload directory exists
        self._ensure_upload_dir()

        # Data source registry (in production, this would be in database)
        self.data_sources = {}

        # Initialize demo data for testing
        self._initialize_demo_data()

        # Initialize Cube.js connector service
        self.cube_connector = CubeConnectorService()
        self.ai_schema_service = AISchemaService()  # Add AI schema service

        # Database connector configurations (now using Cube.js)
        self.db_connectors = {
            "postgresql": self._create_cube_connector,
            "mysql": self._create_cube_connector,
            "sqlserver": self._create_cube_connector,
            "snowflake": self._create_cube_connector,
            "bigquery": self._create_cube_connector,
            "redshift": self._create_cube_connector,
        }

    async def test_database_connection(
        self, connection_request: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Test database connection without storing credentials"""
        try:
            logger.info(
                f"üîå Testing database connection: {connection_request.get('type')}"
            )

            db_type = connection_request.get("type", "").lower()

            if db_type not in self.db_connectors:
                return {
                    "success": False,
                    "error": f"Unsupported database type: {db_type}",
                }

            # Test connection using Cube.js connector directly
            try:
                test_result = await self.cube_connector.test_connection(
                    connection_request
                )
                return test_result

            except Exception as e:
                logger.error(f"‚ùå Database connection test error: {str(e)}")
                return {"success": False, "error": f"Connection test failed: {str(e)}"}

        except Exception as e:
            logger.error(f"‚ùå Database connection test failed: {str(e)}")
            return {"success": False, "error": str(e)}

    async def store_database_connection(
        self, connection_request: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Store database connection configuration"""
        try:
            logger.info(
                f"üíæ Storing database connection: {connection_request.get('type')}"
            )

            # Generate unique ID for the connection
            connection_id = (
                f"db_{connection_request.get('type')}_{int(datetime.now().timestamp())}"
            )

            # Prepare in-memory representation upfront to avoid unbound variable
            connection_data = {
                "id": connection_id,
                "type": "database",
                "db_type": connection_request.get("type"),
                "name": connection_request.get(
                    "name", f"{connection_request.get('type')}_connection"
                ),
                "config": connection_request,
                "created_at": datetime.now().isoformat(),
                "status": "connected",
                "connection_type": "database",
            }

            # Store connection info in database
            try:
                from app.modules.data.models import DataSource
                from app.db.session import get_async_session

                async with get_async_session() as db:
                    # Create new data source record
                    new_source = DataSource(
                        id=connection_id,
                        name=connection_request.get("name")
                        or f"{connection_request.get('type')}_connection",
                        type="database",
                        format=connection_request.get("type"),
                        db_type=connection_request.get("type"),
                        size=0,  # Database connections don't have file size
                        row_count=0,  # Will be populated when schema is fetched
                        schema=json.dumps(
                            {
                                "type": connection_request.get("type"),
                                "host": connection_request.get("host"),
                                "port": connection_request.get("port"),
                                "database": connection_request.get("database"),
                                "username": connection_request.get("username"),
                                "ssl_mode": connection_request.get(
                                    "ssl_mode", "prefer"
                                ),
                                "connection_string": connection_request.get("uri"),
                                "encrypt": connection_request.get("encrypt", False),
                            }
                        ),
                        connection_config=json.dumps(connection_request),
                        metadata=json.dumps(
                            {
                                "connection_type": "database",
                                "status": "connected",
                                "created_at": datetime.now().isoformat(),
                            }
                        ),
                        is_active=True,
                        created_at=datetime.now(),
                        updated_at=datetime.now(),
                        last_accessed=datetime.now(),
                    )

                    db.add(new_source)
                    await db.commit()
                    await db.refresh(new_source)

                    logger.info(
                        f"‚úÖ Database connection saved to database: {connection_id}"
                    )
                    # Keep in-memory registry aligned
                    connection_data.update(
                        {"name": new_source.name, "db_type": new_source.db_type}
                    )
            except Exception as db_error:
                logger.error(f"‚ùå Failed to save to database: {str(db_error)}")
                # Fallback to memory storage (connection_data already prepared)
            self.data_sources[connection_id] = connection_data

            # Also register with Cube.js if available
            try:
                cube_result = await self.cube_connector.create_cube_data_source(
                    connection_request
                )
                if cube_result["success"]:
                    logger.info(
                        f"‚úÖ Database connection registered with Cube.js: {connection_id}"
                    )
            except Exception as cube_error:
                logger.warning(
                    f"‚ö†Ô∏è Cube.js integration failed for {connection_id}: {str(cube_error)}"
                )

            logger.info(f"‚úÖ Database connection stored: {connection_id}")

            return {
                "success": True,
                "data_source_id": connection_id,
                "connection_info": {
                    "id": connection_id,
                    "type": "database",
                    "db_type": connection_request.get("type"),
                    "name": connection_request.get(
                        "name", f"{connection_request.get('type')}_connection"
                    ),
                    "status": "connected",
                },
            }

        except Exception as e:
            logger.error(f"‚ùå Failed to store database connection: {str(e)}")
            return {"success": False, "error": str(e)}

    async def _read_file_data(
        self, file_path: str, file_format: str, limit: int = 100
    ) -> List[Dict[str, Any]]:
        """Read data from a file based on its format"""
        try:
            logger.info(f"üìÅ Reading {file_format} file: {file_path}")

            if file_format == "csv":
                import pandas as pd

                df = pd.read_csv(file_path, nrows=limit)
                return df.to_dict("records")
            elif file_format == "xlsx":
                import pandas as pd

                df = pd.read_excel(file_path, nrows=limit)
                return df.to_dict("records")
            elif file_format == "json":
                import json

                with open(file_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                if isinstance(data, list):
                    return data[:limit]
                else:
                    return [data]
            elif file_format == "parquet":
                import pandas as pd

                df = pd.read_parquet(file_path)
                return df.head(limit).to_dict("records")
            else:
                logger.warning(f"‚ö†Ô∏è Unsupported file format: {file_format}")
                return []

        except Exception as e:
            logger.error(f"‚ùå Failed to read file {file_path}: {str(e)}")
            return []

    async def execute_query_on_source(
        self, source_id: str, query: str
    ) -> Dict[str, Any]:
        """Execute a custom query on a data source"""
        try:
            logger.info(f"üîç Executing query on source: {source_id}")
            logger.info(f"üîç Query: {query}")

            # Get the data source
            source = await self.get_data_source_by_id(source_id)
            if not source:
                return {"success": False, "error": f"Data source {source_id} not found"}

            # For demo data sources, implement basic query parsing
            if source.get("source") == "demo_data" or source.get("id", "").startswith(
                "demo_"
            ):
                data = source.get("sample_data", [])

                # Simple query parsing for demo purposes
                if "count" in query.lower():
                    # Count query
                    result = len(data)
                    return {
                        "success": True,
                        "data": [{"count": result}],
                        "total_rows": 1,
                        "query_type": "count",
                    }
                elif "select" in query.lower() and "from" in query.lower():
                    # Basic SELECT query simulation
                    # This is a simplified implementation for demo purposes
                    try:
                        # Parse basic SELECT * FROM table
                        if "select *" in query.lower():
                            return {
                                "success": True,
                                "data": data,
                                "total_rows": len(data),
                                "query_type": "select_all",
                            }
                        else:
                            # For other queries, return sample data
                            return {
                                "success": True,
                                "data": data[:10],  # Limit to 10 rows
                                "total_rows": len(data[:10]),
                                "query_type": "select_limited",
                            }
                    except Exception as parse_error:
                        logger.warning(
                            f"Query parsing failed, returning sample data: {parse_error}"
                        )
                        return {
                            "success": True,
                            "data": data[:10],
                            "total_rows": len(data[:10]),
                            "query_type": "fallback",
                        }
                else:
                    # Default: return sample data
                    return {
                        "success": True,
                        "data": data,
                        "total_rows": len(data),
                        "query_type": "default",
                    }

            # For file sources, this would parse and execute the query
            elif source.get("type") == "file":
                return {
                    "success": False,
                    "error": "File query execution not yet implemented",
                }

            # For database sources, this would execute SQL
            elif source.get("type") == "database":
                return {
                    "success": False,
                    "error": "Database query execution not yet implemented",
                }

            else:
                return {
                    "success": False,
                    "error": f"Unsupported data source type: {source.get('type')}",
                }

        except Exception as e:
            logger.error(f"‚ùå Failed to execute query on source {source_id}: {str(e)}")
            return {"success": False, "error": str(e)}

    async def get_source_schema(self, source_id: str) -> Dict[str, Any]:
        """Get schema information for a specific data source"""
        try:
            logger.info(f"üîç Getting schema for source: {source_id}")

            # Get the data source
            source = await self.get_data_source_by_id(source_id)
            if not source:
                return {"success": False, "error": f"Data source {source_id} not found"}

            # Return schema information
            schema = source.get("schema", {})
            # Parse JSON string schemas from database if necessary
            if isinstance(schema, str):
                try:
                    schema = json.loads(schema)
                except json.JSONDecodeError:
                    logger.warning(
                        "‚ö†Ô∏è Stored schema is a string but not valid JSON; returning empty schema"
                    )
                    schema = {}
            if schema:
                logger.info(f"‚úÖ Returning schema for source {source_id}")
                return {
                    "success": True,
                    "schema": schema,
                    "source_id": source_id,
                    "source_name": source.get("name", "Unknown"),
                    "source_type": source.get("type", "Unknown"),
                }
            else:
                return {
                    "success": False,
                    "error": f"No schema available for data source {source_id}",
                }

        except Exception as e:
            logger.error(f"‚ùå Failed to get schema for source {source_id}: {str(e)}")
            return {"success": False, "error": str(e)}

    async def get_data_from_source(
        self, source_id: str, limit: int = 100
    ) -> Dict[str, Any]:
        """Get data from a specific data source"""
        try:
            logger.info(f"üîç Getting data from source: {source_id}")

            # Get the data source
            source = await self.get_data_source_by_id(source_id)
            if not source:
                return {"success": False, "error": f"Data source {source_id} not found"}

            # For demo data sources, return sample data
            if source.get("source") == "demo_data" or source.get("id", "").startswith(
                "demo_"
            ):
                sample_data = source.get("sample_data", [])
                logger.info(
                    f"‚úÖ Returning {len(sample_data)} sample data rows from demo source"
                )
                return {
                    "success": True,
                    "data": sample_data,
                    "total_rows": len(sample_data),
                    "source": source,
                }

            # For file sources, read the actual file
            elif source.get("type") == "file":
                file_path = source.get("file_path")
                if not file_path:
                    return {
                        "success": False,
                        "error": "No file path available for this data source",
                    }

                # Read file data based on format
                file_format = source.get("format", "csv").lower()
                data = await self._read_file_data(file_path, file_format, limit)

                return {
                    "success": True,
                    "data": data,
                    "total_rows": len(data),
                    "source": source,
                }

            # For database sources, this would query the database
            elif source.get("type") == "database":
                return {
                    "success": False,
                    "error": "Database data retrieval not yet implemented",
                }

            else:
                return {
                    "success": False,
                    "error": f"Unsupported data source type: {source.get('type')}",
                }

        except Exception as e:
            logger.error(f"‚ùå Failed to get data from source {source_id}: {str(e)}")
            return {"success": False, "error": str(e)}

    async def get_data_source_by_id(self, source_id: str) -> Optional[Dict[str, Any]]:
        """Get a specific data source by ID"""
        try:
            # First check in-memory demo sources
            if source_id in self.data_sources:
                logger.info(f"‚úÖ Found data source {source_id} in demo sources")
                return self.data_sources[source_id]

            # Then check database
            from app.modules.data.models import DataSource
            from app.db.session import get_async_session

            async with get_async_session() as db:
                from sqlalchemy import select

                query = select(DataSource).where(
                    DataSource.id == source_id, DataSource.is_active
                )
                result = await db.execute(query)
                source = result.scalar_one_or_none()

                if source:
                    source_dict = {
                        "id": source.id,
                        "name": source.name,
                        "type": source.type,
                        "format": source.format,
                        "db_type": source.db_type,
                        "size": source.size,
                        "row_count": source.row_count,
                        "schema": source.schema,
                        "created_at": source.created_at.isoformat()
                        if source.created_at
                        else None,
                        "updated_at": source.updated_at.isoformat()
                        if source.updated_at
                        else None,
                        "is_active": source.is_active,
                        "last_accessed": source.last_accessed.isoformat()
                        if source.last_accessed
                        else None,
                    }
                    logger.info(f"‚úÖ Found data source {source_id} in database")
                    return source_dict

                logger.warning(
                    f"‚ö†Ô∏è Data source {source_id} not found in database or demo sources"
                )
                return None

        except Exception as e:
            logger.error(f"‚ùå Failed to get data source {source_id}: {str(e)}")
            # Fallback to in-memory sources
            return self.data_sources.get(source_id)

    async def get_data_sources(
        self, offset: int = 0, limit: int = 100
    ) -> List[Dict[str, Any]]:
        """Get all data sources with pagination from database"""
        try:
            logger.info(f"üîç Getting data sources (offset: {offset}, limit: {limit})")
            logger.info(f"üîç Available demo sources: {list(self.data_sources.keys())}")

            from app.modules.data.models import DataSource
            from app.db.session import get_async_session

            async with get_async_session() as db:
                # Query database for data sources using async session
                from sqlalchemy import select, func

                # Count total
                count_query = select(func.count(DataSource.id)).where(
                    DataSource.is_active
                )
                total_result = await db.execute(count_query)
                total_result.scalar()

                # Get paginated results
                query = (
                    select(DataSource)
                    .where(DataSource.is_active)
                    .offset(offset)
                    .limit(limit)
                )
                result = await db.execute(query)
                sources = result.scalars().all()

                # Convert to dictionary format
                result_list = []
                for source in sources:
                    source_dict = {
                        "id": source.id,
                        "name": source.name,
                        "type": source.type,
                        "format": source.format,
                        "db_type": source.db_type,
                        "size": source.size,
                        "row_count": source.row_count,
                        "schema": source.schema,
                        "created_at": source.created_at.isoformat()
                        if source.created_at
                        else None,
                        "updated_at": source.updated_at.isoformat()
                        if source.updated_at
                        else None,
                        "is_active": source.is_active,
                        "last_accessed": source.last_accessed.isoformat()
                        if source.last_accessed
                        else None,
                    }
                    result_list.append(source_dict)

                logger.info(
                    f"‚úÖ Retrieved {len(result_list)} data sources from database"
                )

                # Always include demo data sources for testing
                demo_sources = list(self.data_sources.values())
                logger.info(f"üîç Adding {len(demo_sources)} demo sources")
                all_sources = result_list + demo_sources
                logger.info(f"üîç Total sources after combining: {len(all_sources)}")

                # Apply pagination to combined sources
                result = all_sources[offset : offset + limit]
                logger.info(f"üîç Returning {len(result)} sources after pagination")
                return result

        except Exception as e:
            logger.error(f"‚ùå Failed to get data sources from database: {str(e)}")
            import traceback

            logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
            # Fallback to in-memory sources
            sources = list(self.data_sources.values())
            logger.info(f"‚úÖ Using {len(sources)} in-memory demo data sources")
            return sources[offset : offset + limit]

    async def _save_data_source_to_db(self, data_source: Dict[str, Any]) -> bool:
        """Save data source to database"""
        try:
            from app.modules.data.models import DataSource
            from app.db.session import get_async_session

            async with get_async_session() as db:
                # Check if data source already exists
                from sqlalchemy import select

                existing_query = select(DataSource).where(
                    DataSource.id == data_source["id"]
                )
                existing_result = await db.execute(existing_query)
                existing = existing_result.scalar_one_or_none()

                if existing:
                    # Update existing
                    existing.name = data_source["name"]
                    existing.type = data_source["type"]
                    existing.format = data_source.get("format")
                    existing.size = data_source.get("size")
                    existing.row_count = data_source.get("row_count")
                    existing.schema = data_source.get("schema")
                    existing.file_path = data_source.get("file_path")
                    existing.original_filename = data_source.get("original_filename")
                    existing.updated_at = datetime.now()
                    logger.info(
                        f"‚úÖ Updated data source {data_source['id']} in database."
                    )
                else:
                    # Create new
                    new_data_source = DataSource(
                        id=data_source["id"],
                        name=data_source["name"],
                        type=data_source["type"],
                        format=data_source.get("format"),
                        size=data_source.get("size"),
                        row_count=data_source.get("row_count"),
                        schema=data_source.get("schema"),
                        file_path=data_source.get("file_path"),
                        original_filename=data_source.get("original_filename"),
                        user_id=self._get_current_user_id(),  # Get from auth context
                        tenant_id="default",
                    )
                    db.add(new_data_source)
                    logger.info(
                        f"‚úÖ Saved new data source {data_source['id']} to database."
                    )

                await db.commit()
                return True

        except Exception as e:
            logger.error(f"‚ùå Failed to save data source to database: {str(e)}")
            return False

    async def process_uploaded_file(
        self,
        file_path: str,
        original_filename: str,
        options: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """Process uploaded file and extract data"""
        try:
            logger.info(f"üìÅ Processing uploaded file: {original_filename}")

            if options is None:
                options = {}

            file_extension = self._get_file_extension(original_filename)

            if file_extension not in self.supported_formats:
                raise ValueError(f"Unsupported file format: {file_extension}")

            # Process file based on extension
            if file_extension == "csv":
                data, schema = await self._process_csv_file(
                    file_path,
                    options.get("delimiter", ","),
                    options.get("encoding", "utf-8"),
                )
            elif file_extension == "tsv":
                data, schema = await self._process_csv_file(file_path, "\t", "utf-8")
            elif file_extension == "parquet":
                data, schema = await self._process_parquet_file(file_path)
            elif file_extension == "json":
                data, schema = await self._process_json_file(file_path)
            else:
                raise ValueError(f"Unsupported file format: {file_extension}")

            # Enhance schema with AI insights
            enhanced_schema = await self.ai_schema_service.generate_enhanced_schema(
                data, schema, original_filename, "file"
            )

            # Generate data source metadata
            data_source = {
                "id": f"file_{int(datetime.now().timestamp())}",
                "name": original_filename,
                "type": "file",
                "format": file_extension,
                "size": os.path.getsize(file_path),
                "uploaded_at": datetime.now().isoformat(),
                "schema": enhanced_schema,
                "row_count": len(data),
                "file_path": file_path,
                "preview": data[:10]
                if len(data) > 10
                else data,  # First 10 rows for preview
                # Add fields expected by frontend
                "uuid_filename": f"file_{int(datetime.now().timestamp())}_{original_filename}",
                "content_type": f"application/{file_extension}",
                "storage_type": "local",
            }

            # Store in registry
            self.data_sources[data_source["id"]] = {**data_source, "data": data}

            # Save to database
            await self._save_data_source_to_db(data_source)

            logger.info(
                f"‚úÖ File processed successfully: {len(data)} rows, {len(enhanced_schema['columns'])} columns"
            )

            return {
                "success": True,
                "data_source": data_source,
                "data": data if options.get("include_data") else None,
            }

        except Exception as error:
            logger.error(f"‚ùå File processing failed: {str(error)}")

            # Clean up file on error
            if os.path.exists(file_path):
                os.unlink(file_path)

            return {"success": False, "error": str(error)}

    async def upload_file(
        self,
        file_content: bytes,
        filename: str,
        options: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """Upload and process a file from content"""
        try:
            logger.info(f"üìÅ File upload request: {filename}")

            if options is None:
                options = {}

            # Validate file size
            if len(file_content) > self.max_file_size:
                raise ValueError(
                    f"File too large. Maximum size: {self.max_file_size / (1024 * 1024):.1f}MB"
                )

            # Validate file format
            file_extension = self._get_file_extension(filename)
            if file_extension not in self.supported_formats:
                raise ValueError(f"Unsupported file format: {file_extension}")

            # Create unique filename and save to upload directory
            timestamp = int(datetime.now().timestamp())
            unique_filename = f"file_{timestamp}_{filename}"
            file_path = os.path.join(self.upload_dir, unique_filename)

            # Ensure upload directory exists
            self._ensure_upload_dir()

            # Save file content
            with open(file_path, "wb") as f:
                f.write(file_content)

            logger.info(f"üíæ File saved to: {file_path}")

            # Process the uploaded file
            result = await self.process_uploaded_file(file_path, filename, options)

            if result["success"]:
                logger.info(f"‚úÖ File upload completed successfully: {filename}")
                return result
            else:
                # Clean up file if processing failed
                if os.path.exists(file_path):
                    os.unlink(file_path)
                return result

        except Exception as error:
            logger.error(f"‚ùå File upload failed: {str(error)}")
            return {"success": False, "error": str(error)}

    async def _process_csv_file(
        self, file_path: str, delimiter: str = ",", encoding: str = "utf-8"
    ) -> tuple:
        """Process CSV/TSV files with enhanced auto-detection"""
        try:
            # Try to auto-detect delimiter if not specified
            if delimiter == ",":
                detected_delimiter = self._auto_detect_delimiter(file_path)
                if detected_delimiter:
                    delimiter = detected_delimiter
                    logger.info(f"üîç Auto-detected delimiter: '{delimiter}'")

            # Try to auto-detect encoding if not specified
            if encoding == "utf-8":
                detected_encoding = self._auto_detect_encoding(file_path)
                if detected_encoding:
                    encoding = detected_encoding
                    logger.info(f"üîç Auto-detected encoding: {encoding}")

            # Read CSV with pandas, handling newlines properly
            df = pd.read_csv(file_path, delimiter=delimiter, encoding=encoding)

            # Convert to list of dictionaries
            data = df.to_dict("records")

            # Clean data (convert NaN to None)
            for row in data:
                for key, value in row.items():
                    if pd.isna(value):
                        row[key] = None

            # Infer schema
            schema = self._infer_schema_from_dataframe(df)

            return data, schema

        except Exception as error:
            raise Exception(f"CSV processing failed: {str(error)}")

    async def _process_parquet_file(self, file_path: str) -> tuple:
        """Process Parquet files"""
        try:
            # Read Parquet with pyarrow for better performance
            import pyarrow.parquet as pq

            # Read data directly
            df = pq.read_table(file_path).to_pandas()

            # Convert to list of dictionaries
            data = df.to_dict("records")

            # Clean data (convert NaN to None)
            for row in data:
                for key, value in row.items():
                    if pd.isna(value):
                        row[key] = None

            # Infer schema
            schema = self._infer_schema_from_dataframe(df)

            # Add basic parquet info if available
            try:
                parquet_file = pq.ParquetFile(file_path)
                metadata = parquet_file.metadata
                schema["parquet_metadata"] = {
                    "row_groups": getattr(metadata, "num_row_groups", 0),
                    "total_rows": getattr(metadata, "num_rows", len(data)),
                    "created_by": getattr(metadata, "created_by", "unknown"),
                    "schema_version": getattr(metadata, "schema_version", "unknown"),
                }
            except Exception:
                # If metadata access fails, add basic info
                schema["parquet_metadata"] = {
                    "row_groups": 1,
                    "total_rows": len(data),
                    "created_by": "unknown",
                    "schema_version": "unknown",
                }

            return data, schema

        except Exception as error:
            raise Exception(f"Parquet processing failed: {str(error)}")

    async def _process_excel_file(
        self, file_path: str, sheet_name: Optional[str] = None
    ) -> tuple:
        """Process Excel files"""
        try:
            # Read Excel with pandas
            if sheet_name:
                df = pd.read_excel(file_path, sheet_name=sheet_name)
            else:
                df = pd.read_excel(file_path)

            # Convert to list of dictionaries
            data = df.to_dict("records")

            # Clean data (convert NaN to None)
            for row in data:
                for key, value in row.items():
                    if pd.isna(value):
                        row[key] = None

            # Infer schema
            schema = self._infer_schema_from_dataframe(df)

            return data, schema

        except Exception as error:
            raise Exception(f"Excel processing failed: {str(error)}")

    async def _process_json_file(self, file_path: str) -> tuple:
        """Process JSON files"""
        try:
            with open(file_path, "r", encoding="utf-8") as file:
                json_data = json.load(file)

            # Handle different JSON structures
            if isinstance(json_data, list):
                data = json_data
            elif isinstance(json_data, dict):
                if "data" in json_data and isinstance(json_data["data"], list):
                    data = json_data["data"]
                else:
                    data = [json_data]
            else:
                raise ValueError(
                    "Invalid JSON structure - expected array or object with data array"
                )

            # Convert to DataFrame for schema inference
            df = pd.DataFrame(data)

            # Infer schema
            schema = self._infer_schema_from_dataframe(df)

            return data, schema

        except Exception as error:
            raise Exception(f"JSON processing failed: {str(error)}")

    def _infer_schema_from_dataframe(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Infer schema from pandas DataFrame"""
        columns = []
        types = {}
        statistics = {}

        for column in df.columns:
            series = df[column]

            # Determine data type
            if pd.api.types.is_numeric_dtype(series):
                if pd.api.types.is_integer_dtype(series):
                    data_type = "integer"
                else:
                    data_type = "number"

                # Calculate statistics for numeric columns
                statistics[column] = {
                    "min": float(series.min()) if not pd.isna(series.min()) else None,
                    "max": float(series.max()) if not pd.isna(series.max()) else None,
                    "mean": float(series.mean())
                    if not pd.isna(series.mean())
                    else None,
                    "null_count": int(series.isnull().sum()),
                }
            elif pd.api.types.is_datetime64_any_dtype(series):
                data_type = "date"
                statistics[column] = {"null_count": int(series.isnull().sum())}
            elif pd.api.types.is_bool_dtype(series):
                data_type = "boolean"
                statistics[column] = {"null_count": int(series.isnull().sum())}
            else:
                data_type = "string"

                # Calculate statistics for string columns
                non_null_series = series.dropna()
                statistics[column] = {
                    "unique_count": int(series.nunique()),
                    "max_length": int(non_null_series.astype(str).str.len().max())
                    if len(non_null_series) > 0
                    else 0,
                    "null_count": int(series.isnull().sum()),
                }

            types[column] = data_type
            columns.append(
                {
                    "name": column,
                    "type": data_type,
                    "nullable": bool(
                        series.isnull().any()
                    ),  # Convert numpy.bool_ to Python bool
                    "statistics": statistics[column],
                }
            )

        return {
            "columns": columns,
            "types": types,
            "row_count": int(len(df)),  # Convert numpy.int64 to Python int
            "inferred_at": datetime.now().isoformat(),
        }

    def _parse_database_uri(self, uri: str) -> Dict[str, Any]:
        """Parse database URI into connection parameters"""
        try:
            from urllib.parse import urlparse

            parsed = urlparse(uri)

            # Map protocol to database type
            protocol_map = {
                "postgres": "postgresql",
                "postgresql": "postgresql",
                "mysql": "mysql",
                "sqlserver": "sqlserver",
                "mssql": "sqlserver",
            }

            db_type = protocol_map.get(parsed.scheme, parsed.scheme)

            return {
                "type": db_type,
                "host": parsed.hostname,
                "port": parsed.port,
                "database": parsed.path.lstrip("/") if parsed.path else "",
                "username": parsed.username,
                "password": parsed.password,
                "uri": uri,
            }
        except Exception as e:
            raise ValueError(f"Invalid database URI format: {str(e)}")

    async def create_database_connection(
        self, config: Dict[str, Any], tenant_id: str = "default"
    ) -> Dict[str, Any]:
        """Create database connection using Cube.js connectors"""
        try:
            # Handle URI-based connections
            if config.get("uri"):
                logger.info("üîå Parsing database URI connection")
                parsed_config = self._parse_database_uri(config["uri"])
                # Merge with original config, keeping name if provided
                config = {
                    **parsed_config,
                    **{k: v for k, v in config.items() if k != "uri"},
                }
                logger.info(
                    f"üîå Parsed URI to: {config.get('type')} at {config.get('host')}:{config.get('port')}"
                )

            logger.info(
                f"üîå Creating database connection via Cube.js: {config.get('type')}"
            )

            db_type = config.get("type")
            if db_type not in self.db_connectors:
                # Get supported databases from Cube.js
                supported = self.cube_connector.get_supported_databases()
                supported_types = [
                    db["type"] for db in supported["supported_databases"]
                ]
                raise ValueError(
                    f"Unsupported database type: {db_type}. Supported: {supported_types}"
                )

            # Create connection using Cube.js connector
            cube_result = await self.cube_connector.create_cube_data_source(
                config, tenant_id
            )

            if not cube_result["success"]:
                raise Exception(cube_result["error"])

            cube_data_source = cube_result["data_source"]

            # Generate our data source metadata
            data_source = {
                "id": cube_data_source["id"],
                "name": cube_data_source["name"],
                "type": "database",
                "db_type": cube_data_source["db_type"],
                "host": config.get("host"),
                "database": config.get("database"),
                "created_at": datetime.now().isoformat(),
                "tenant_id": tenant_id,
                "cube_integration": True,
                "driver": cube_data_source["driver"],
                "status": cube_data_source["status"],
            }

            # Store in registry AND database
            self.data_sources[data_source["id"]] = {
                **data_source,
                "cube_data_source": cube_data_source,
            }

            # Also save to database for persistence
            try:
                from app.modules.data.models import DataSource as DataSourceModel
                from app.db.session import get_db

                # Create data source model instance
                db_data_source = DataSourceModel(
                    id=data_source["id"],
                    name=data_source["name"],
                    type="database",
                    db_type=data_source.get("db_type"),
                    connection_config=json.dumps(config),
                    metadata=json.dumps(data_source),
                    is_active=True,
                )

                # Add to session and commit
                db = get_db()
                await db.add(db_data_source)
                logger.info(f"‚úÖ Data source saved to database: {data_source['id']}")
            except Exception as db_error:
                logger.warning(f"‚ö†Ô∏è Could not save to database: {db_error}")
                # Continue anyway since we have in-memory storage

            logger.info(
                f"‚úÖ Cube.js database connection created successfully: {data_source['id']}"
            )

            return {
                "success": True,
                "data_source": {
                    "id": data_source["id"],
                    "name": data_source["name"],
                    "type": data_source["type"],
                    "db_type": data_source["db_type"],
                    "created_at": data_source["created_at"],
                    "cube_integration": True,
                    "driver": data_source["driver"],
                    "status": data_source["status"],
                },
            }

        except Exception as error:
            logger.error(f"‚ùå Cube.js database connection failed: {str(error)}")
            return {"success": False, "error": str(error)}

    async def query_data_source(
        self, data_source_id: str, query: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Query data from data source"""
        try:
            data_source = self.data_sources.get(data_source_id)
            if not data_source:
                raise ValueError(f"Data source not found: {data_source_id}")

            if query is None:
                query = {}

            if data_source["type"] == "file":
                return await self._query_file_data_source(data_source, query)
            elif data_source["type"] == "database":
                return await self._query_database_data_source(data_source, query)
            else:
                raise ValueError(f"Unsupported data source type: {data_source['type']}")

        except Exception as error:
            logger.error(f"‚ùå Data source query failed: {str(error)}")
            return {"success": False, "error": str(error)}

    async def _query_file_data_source(
        self, data_source: Dict[str, Any], query: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Query file-based data source"""
        data = data_source.get("data", [])

        # Apply filters
        filters = query.get("filters", [])
        for filter_item in filters:
            column = filter_item.get("column")
            operator = filter_item.get("operator")
            value = filter_item.get("value")

            if operator == "equals":
                data = [
                    row
                    for row in data
                    if str(row.get(column, "")).lower() == str(value).lower()
                ]
            elif operator == "contains":
                data = [
                    row
                    for row in data
                    if str(value).lower() in str(row.get(column, "")).lower()
                ]
            elif operator == "greater_than":
                data = [
                    row for row in data if float(row.get(column, 0) or 0) > float(value)
                ]
            elif operator == "less_than":
                data = [
                    row for row in data if float(row.get(column, 0) or 0) < float(value)
                ]

        # Apply sorting
        sort_config = query.get("sort")
        if sort_config:
            column = sort_config.get("column")
            direction = sort_config.get("direction", "asc")
            reverse = direction == "desc"

            try:
                data = sorted(data, key=lambda x: x.get(column, ""), reverse=reverse)
            except:
                # If sorting fails, continue without sorting
                pass

        # Apply pagination
        offset = query.get("offset", 0)
        limit = query.get("limit", 1000)
        total_rows = len(data)
        paginated_data = data[offset : offset + limit]

        return {
            "success": True,
            "data": paginated_data,
            "total_rows": total_rows,
            "offset": offset,
            "limit": limit,
            "schema": data_source.get("schema"),
        }

    async def _query_database_data_source(
        self, data_source: Dict[str, Any], query: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Query database data source through Cube.js"""
        try:
            if not data_source.get("cube_integration"):
                return {
                    "success": False,
                    "error": "Database source not integrated with Cube.js",
                }

            cube_data_source = data_source.get("cube_data_source")
            if not cube_data_source:
                return {"success": False, "error": "Cube.js data source not found"}

            # Convert our query format to Cube.js query format
            cube_query = self._convert_to_cube_query(query, cube_data_source)

            # Execute query through Cube.js
            result = await self.cube_connector.query_cube_data_source(
                data_source["id"], cube_query, data_source.get("tenant_id", "default")
            )

            return result

        except Exception as error:
            logger.error(f"‚ùå Database query through Cube.js failed: {str(error)}")
            return {"success": False, "error": str(error)}

    async def get_data_source(self, data_source_id: str) -> Dict[str, Any]:
        """Get a specific data source by ID"""
        try:
            data_source = self.data_sources.get(data_source_id)
            if data_source:
                return {"success": True, "data_source": data_source}
            else:
                return {"success": False, "error": "Data source not found"}
        except Exception as e:
            logger.error(f"‚ùå Failed to get data source: {str(e)}")
            return {"success": False, "error": str(e)}

    def list_data_sources(self) -> Dict[str, Any]:
        """List all data sources"""
        sources = []
        for source in self.data_sources.values():
            metadata = {
                key: value
                for key, value in source.items()
                if key not in ["data", "connection"]
            }
            sources.append(metadata)

        return {"success": True, "data_sources": sources, "count": len(sources)}

    def delete_data_source(self, data_source_id: str) -> Dict[str, Any]:
        """Delete data source"""
        try:
            data_source = self.data_sources.get(data_source_id)

            # Attempt DB deletion first (if persisted)
            try:
                from app.modules.data.models import DataSource
                from app.modules.projects.models import ProjectDataSource
                from app.db.session import get_async_session
                import asyncio

                async def _delete_from_db():
                    async with get_async_session() as db:
                        from sqlalchemy import delete

                        # Remove project links
                        try:
                            await db.execute(
                                delete(ProjectDataSource).where(
                                    ProjectDataSource.data_source_id == data_source_id
                                )
                            )
                        except Exception:
                            pass
                        # Remove data source row
                        await db.execute(
                            delete(DataSource).where(DataSource.id == data_source_id)
                        )
                        await db.commit()

                # Run async deletion in current loop if available
                try:
                    loop = asyncio.get_event_loop()
                    if loop.is_running():
                        asyncio.ensure_future(_delete_from_db())
                        # fire-and-forget
                    else:
                        loop.run_until_complete(_delete_from_db())
                except RuntimeError:
                    # No loop, run new loop
                    asyncio.run(_delete_from_db())
            except Exception as db_del_err:
                logger.warning(
                    f"DB deletion for data source {data_source_id} skipped/failed: {db_del_err}"
                )

            # In-memory cleanup
            if data_source:
                # Clean up file if it's a file-based source
                if data_source.get("type") == "file" and "file_path" in data_source:
                    file_path = data_source["file_path"]
                    if os.path.exists(file_path):
                        os.unlink(file_path)
                # Close database connection if tracked
                if (
                    data_source.get("type") == "database"
                    and "connection" in data_source
                ):
                    connection = data_source["connection"]
                    if hasattr(connection, "close"):
                        try:
                            connection.close()
                        except Exception:
                            pass
                self.data_sources.pop(data_source_id, None)

            return {"success": True, "message": "Data source deleted successfully"}
        except Exception as e:
            logger.error(f"Failed to delete data source {data_source_id}: {e}")
            return {"success": False, "error": str(e)}

    async def generate_data_insights(self, data_source_id: str) -> Dict[str, Any]:
        """Generate AI insights for a data source"""
        try:
            logger.info(f"üîç Generating AI insights for data source: {data_source_id}")

            # Get the data source
            data_source = self.data_sources.get(data_source_id)
            if not data_source:
                return {"success": False, "error": "Data source not found"}

            # Get data and schema
            data = data_source.get("data", [])
            schema = data_source.get("schema", {})
            name = data_source.get("name", "Unknown")

            # Generate insights using AI
            insights_result = await self.ai_schema_service.generate_data_insights(
                data, schema, name
            )

            return insights_result

        except Exception as e:
            logger.error(f"‚ùå Failed to generate insights: {str(e)}")
            return {"success": False, "error": str(e)}

    async def get_database_schema(self, data_source_id: str) -> Dict[str, Any]:
        """Get database schema information for a connected database"""
        try:
            logger.info(f"üîç Fetching database schema for: {data_source_id}")

            # Get the data source from database
            from app.modules.data.models import DataSource
            from app.db.session import get_async_session

            async with get_async_session() as db:
                from sqlalchemy import select

                query = select(DataSource).where(DataSource.id == data_source_id)
                result = await db.execute(query)
                data_source = result.scalar_one_or_none()

                if not data_source:
                    return {"success": False, "error": "Data source not found"}

                if data_source.type != "database":
                    return {
                        "success": False,
                        "error": "Data source is not a database connection",
                    }

                # Parse the stored schema/config
                try:
                    config = (
                        json.loads(data_source.connection_config)
                        if data_source.connection_config
                        else {}
                    )
                    schema_info = (
                        json.loads(data_source.schema) if data_source.schema else {}
                    )
                except json.JSONDecodeError:
                    config = {}
                    schema_info = {}

                # Try to get live schema from the database
                try:
                    live_schema = await self._fetch_live_database_schema(config)
                    if live_schema["success"]:
                        # Update the stored schema with live data
                        updated_schema = {
                            **schema_info,
                            "tables": live_schema.get("tables", []),
                            "schemas": live_schema.get("schemas", []),
                            "last_updated": datetime.now().isoformat(),
                        }

                        # Update the database record
                        data_source.schema = json.dumps(updated_schema)
                        data_source.row_count = live_schema.get("total_rows", 0)
                        data_source.updated_at = datetime.now()
                        await db.commit()

                        return {
                            "success": True,
                            "schema": updated_schema,
                            "data_source": {
                                "id": data_source.id,
                                "name": data_source.name,
                                "type": data_source.type,
                                "db_type": data_source.db_type,
                                "row_count": data_source.row_count,
                            },
                        }
                    else:
                        # Return stored schema if live fetch fails
                        return {
                            "success": True,
                            "schema": schema_info,
                            "data_source": {
                                "id": data_source.id,
                                "name": data_source.name,
                                "type": data_source.type,
                                "db_type": data_source.db_type,
                                "row_count": data_source.row_count,
                            },
                            "warning": "Using cached schema - live fetch failed",
                        }

                except Exception as live_error:
                    logger.warning(f"‚ö†Ô∏è Live schema fetch failed: {str(live_error)}")
                    # Return stored schema
                    return {
                        "success": True,
                        "schema": schema_info,
                        "data_source": {
                            "id": data_source.id,
                            "name": data_source.name,
                            "type": data_source.type,
                            "db_type": data_source.db_type,
                            "row_count": data_source.row_count,
                        },
                        "warning": "Using cached schema - live fetch failed",
                    }

        except Exception as e:
            logger.error(f"‚ùå Failed to get database schema: {str(e)}")
            return {"success": False, "error": str(e)}

    async def _fetch_live_database_schema(
        self, config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Fetch live schema from the database connection"""
        try:
            db_type = config.get("type", "").lower()

            if db_type not in self.db_connectors:
                return {
                    "success": False,
                    "error": f"Unsupported database type: {db_type}",
                }

            # Use Cube.js connector to get schema
            try:
                schema_result = await self.cube_connector.get_database_schema(config)
                if schema_result["success"]:
                    return {
                        "success": True,
                        "tables": schema_result.get("tables", []),
                        "schemas": schema_result.get("schemas", []),
                        "total_rows": schema_result.get("total_rows", 0),
                    }
                else:
                    logger.warning(
                        f"‚ö†Ô∏è Cube.js schema fetch failed: {schema_result.get('error')}"
                    )
                    # Enhanced fallback with more realistic schema structure
                    return await self._get_enhanced_fallback_schema(config)

            except Exception as cube_error:
                logger.warning(f"‚ö†Ô∏è Cube.js schema fetch failed: {str(cube_error)}")
                # Enhanced fallback with more realistic schema structure
                return await self._get_enhanced_fallback_schema(config)

        except Exception as e:
            logger.error(f"‚ùå Live schema fetch failed: {str(e)}")
            return {"success": False, "error": str(e)}

    async def _get_enhanced_fallback_schema(
        self, config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Get enhanced fallback schema with realistic database structure"""
        try:
            db_type = config.get("type", "").lower()
            db_name = config.get("database", "unknown")

            # Create realistic schema based on database type
            if db_type == "postgresql":
                schemas = ["public", "information_schema", "pg_catalog"]
                tables = [
                    {
                        "name": "users",
                        "schema": "public",
                        "columns": [
                            {
                                "name": "id",
                                "type": "integer",
                                "nullable": False,
                                "primary_key": True,
                            },
                            {
                                "name": "username",
                                "type": "varchar(50)",
                                "nullable": False,
                                "unique": True,
                            },
                            {
                                "name": "email",
                                "type": "varchar(100)",
                                "nullable": False,
                                "unique": True,
                            },
                            {
                                "name": "created_at",
                                "type": "timestamp",
                                "nullable": False,
                                "default": "now()",
                            },
                            {
                                "name": "is_active",
                                "type": "boolean",
                                "nullable": False,
                                "default": "true",
                            },
                        ],
                        "rowCount": 0,
                        "description": "User accounts table",
                    },
                    {
                        "name": "orders",
                        "schema": "public",
                        "columns": [
                            {
                                "name": "id",
                                "type": "integer",
                                "nullable": False,
                                "primary_key": True,
                            },
                            {
                                "name": "user_id",
                                "type": "integer",
                                "nullable": False,
                                "foreign_key": "users.id",
                            },
                            {
                                "name": "order_date",
                                "type": "timestamp",
                                "nullable": False,
                                "default": "now()",
                            },
                            {
                                "name": "total_amount",
                                "type": "decimal(10,2)",
                                "nullable": False,
                            },
                            {
                                "name": "status",
                                "type": "varchar(20)",
                                "nullable": False,
                                "default": "pending",
                            },
                        ],
                        "rowCount": 0,
                        "description": "Customer orders table",
                    },
                    {
                        "name": "products",
                        "schema": "public",
                        "columns": [
                            {
                                "name": "id",
                                "type": "integer",
                                "nullable": False,
                                "primary_key": True,
                            },
                            {"name": "name", "type": "varchar(100)", "nullable": False},
                            {"name": "description", "type": "text", "nullable": True},
                            {
                                "name": "price",
                                "type": "decimal(10,2)",
                                "nullable": False,
                            },
                            {
                                "name": "category",
                                "type": "varchar(50)",
                                "nullable": True,
                            },
                            {
                                "name": "stock_quantity",
                                "type": "integer",
                                "nullable": False,
                                "default": "0",
                            },
                        ],
                        "rowCount": 0,
                        "description": "Product catalog table",
                    },
                ]
            elif db_type == "mysql":
                schemas = ["information_schema", "mysql", "performance_schema", "sys"]
                tables = [
                    {
                        "name": "customers",
                        "schema": "information_schema",
                        "columns": [
                            {
                                "name": "id",
                                "type": "int",
                                "nullable": False,
                                "primary_key": True,
                            },
                            {
                                "name": "first_name",
                                "type": "varchar(50)",
                                "nullable": False,
                            },
                            {
                                "name": "last_name",
                                "type": "varchar(50)",
                                "nullable": False,
                            },
                            {
                                "name": "email",
                                "type": "varchar(100)",
                                "nullable": False,
                                "unique": True,
                            },
                            {"name": "phone", "type": "varchar(20)", "nullable": True},
                            {
                                "name": "created_at",
                                "type": "datetime",
                                "nullable": False,
                                "default": "CURRENT_TIMESTAMP",
                            },
                        ],
                        "rowCount": 0,
                        "description": "Customer information table",
                    }
                ]
            else:
                # Generic schema for other database types
                schemas = ["dbo", "information_schema"]
                tables = [
                    {
                        "name": "sample_table",
                        "schema": "dbo",
                        "columns": [
                            {
                                "name": "id",
                                "type": "int",
                                "nullable": False,
                                "primary_key": True,
                            },
                            {"name": "name", "type": "varchar(100)", "nullable": False},
                            {
                                "name": "created_at",
                                "type": "datetime",
                                "nullable": False,
                            },
                        ],
                        "rowCount": 0,
                        "description": "Sample table structure",
                    }
                ]

            return {
                "success": True,
                "tables": tables,
                "schemas": schemas,
                "total_rows": sum(table.get("rowCount", 0) for table in tables),
                "database_info": {
                    "name": db_name,
                    "type": db_type,
                    "host": config.get("host"),
                    "port": config.get("port"),
                    "username": config.get("username"),
                },
                "warning": "Using enhanced fallback schema - live database connection not fully configured",
            }

        except Exception as e:
            logger.error(f"‚ùå Enhanced fallback schema failed: {str(e)}")
            return {"success": False, "error": str(e)}

    # Utility methods
    def _ensure_upload_dir(self):
        """Ensure upload directory exists"""
        if not os.path.exists(self.upload_dir):
            os.makedirs(self.upload_dir, exist_ok=True)

    def _get_file_extension(self, filename: str) -> str:
        """Get file extension"""
        return Path(filename).suffix.lower().lstrip(".")

    # Cube.js connector methods
    async def _create_cube_connector(self, config: Dict[str, Any]):
        """Create database connector through Cube.js"""
        # This is handled by the cube_connector service
        return await self.cube_connector.create_cube_data_source(config)

    def _convert_to_cube_query(
        self, query: Dict[str, Any], cube_data_source: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Convert our query format to Cube.js query format"""
        cube_query = {}

        # Add measures (default to count if none specified)
        cube_query["measures"] = query.get("measures", ["count"])

        # Add dimensions
        if query.get("dimensions"):
            cube_query["dimensions"] = query["dimensions"]

        # Add time dimensions
        if query.get("time_dimensions"):
            cube_query["timeDimensions"] = query["time_dimensions"]

        # Add filters
        if query.get("filters"):
            cube_query["filters"] = self._convert_filters_to_cube_format(
                query["filters"]
            )

        # Add sorting
        if query.get("sort"):
            sort_config = query["sort"]
            cube_query["order"] = [
                [sort_config["column"], sort_config.get("direction", "asc")]
            ]

        # Add limit
        if query.get("limit"):
            cube_query["limit"] = query["limit"]

        return cube_query

    def _convert_filters_to_cube_format(
        self, filters: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """Convert filters to Cube.js format"""
        cube_filters = []

        for filter_item in filters:
            cube_filter = {
                "member": filter_item["column"],
                "operator": self._map_operator_to_cube(filter_item["operator"]),
                "values": [filter_item["value"]],
            }
            cube_filters.append(cube_filter)

        return cube_filters

    def _map_operator_to_cube(self, operator: str) -> str:
        """Map our operators to Cube.js operators"""
        operator_mapping = {
            "equals": "equals",
            "contains": "contains",
            "greater_than": "gt",
            "less_than": "lt",
            "greater_equal": "gte",
            "less_equal": "lte",
        }
        return operator_mapping.get(operator, "equals")

    async def get_supported_databases(self) -> Dict[str, Any]:
        """Get supported database types from Cube.js"""
        return self.cube_connector.get_supported_databases()

    def _auto_detect_delimiter(self, file_path: str) -> str:
        """Auto-detect CSV delimiter"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                first_line = f.readline().strip()

            if not first_line:
                return ","

            # Common delimiters to test
            delimiters = [",", ";", "\t", "|", " "]
            max_fields = 0
            best_delimiter = ","

            for delimiter in delimiters:
                fields = first_line.split(delimiter)
                if len(fields) > max_fields:
                    max_fields = len(fields)
                    best_delimiter = delimiter

            return best_delimiter

        except Exception:
            return ","

    def _auto_detect_encoding(self, file_path: str) -> str:
        """Auto-detect file encoding"""
        try:
            import chardet

            with open(file_path, "rb") as f:
                raw_data = f.read(10000)  # Read first 10KB

            result = chardet.detect(raw_data)
            encoding = result["encoding"]
            confidence = result["confidence"]

            if confidence > 0.7:
                return encoding
            else:
                return "utf-8"

        except ImportError:
            # chardet not available, try common encodings
            encodings = ["utf-8", "latin-1", "cp1252"]

            for encoding in encodings:
                try:
                    with open(file_path, "r", encoding=encoding) as f:
                        f.readline()
                    return encoding
                except UnicodeDecodeError:
                    continue

            return "utf-8"
        except Exception:
            return "utf-8"

    async def get_file_preview(
        self, file_path: str, file_format: str, options: dict = None
    ) -> dict:
        """Get enhanced file preview with auto-detection"""
        try:
            if options is None:
                options = {}

            preview_data = {
                "format": file_format,
                "size": os.path.getsize(file_path),
                "auto_detected_config": {},
                "sample_data": [],
                "schema_preview": {},
                "processing_options": {},
            }

            if file_format in ["csv", "tsv"]:
                # Auto-detect delimiter and encoding
                delimiter = options.get(
                    "delimiter", self._auto_detect_delimiter(file_path)
                )
                encoding = options.get(
                    "encoding", self._auto_detect_encoding(file_path)
                )

                preview_data["auto_detected_config"] = {
                    "delimiter": delimiter,
                    "encoding": encoding,
                }

                # Read first few lines for preview
                df_preview = pd.read_csv(
                    file_path, delimiter=delimiter, encoding=encoding, nrows=10
                )
                preview_data["sample_data"] = df_preview.to_dict("records")
                preview_data["schema_preview"] = {
                    "columns": list(df_preview.columns),
                    "data_types": df_preview.dtypes.to_dict(),
                    "row_count_preview": len(df_preview),
                }

                # Get processing options
                preview_data["processing_options"] = {
                    "delimiters": self.file_processing_configs["csv"]["delimiters"],
                    "encodings": self.file_processing_configs["csv"]["encodings"],
                }

            elif file_format in ["xlsx", "xls"]:
                # Get sheet information
                import openpyxl

                workbook = openpyxl.load_workbook(file_path, read_only=True)
                sheets = workbook.sheetnames

                preview_data["auto_detected_config"] = {
                    "sheets": sheets,
                    "default_sheet": sheets[0] if sheets else None,
                }

                # Read first sheet for preview
                df_preview = pd.read_excel(file_path, sheet_name=sheets[0], nrows=10)
                preview_data["sample_data"] = df_preview.to_dict("records")
                preview_data["schema_preview"] = {
                    "columns": list(df_preview.columns),
                    "data_types": df_preview.dtypes.to_dict(),
                    "row_count_preview": len(df_preview),
                }

                preview_data["processing_options"] = {
                    "sheets": sheets,
                    "header_row_options": [0, 1, 2],
                    "skip_rows_options": [0, 1, 2, 3],
                }

            elif file_format == "parquet":
                # Get parquet metadata
                import pyarrow.parquet as pq

                parquet_file = pq.ParquetFile(file_path)
                metadata = parquet_file.metadata

                preview_data["auto_detected_config"] = {
                    "columns": list(metadata.schema.names),
                    "row_groups": metadata.num_row_groups,
                    "total_rows": metadata.num_rows,
                }

                # Read sample data
                df_preview = parquet_file.read().to_pandas().head(10)
                preview_data["sample_data"] = df_preview.to_dict("records")
                preview_data["schema_preview"] = {
                    "columns": list(df_preview.columns),
                    "data_types": df_preview.dtypes.to_dict(),
                    "row_count_preview": len(df_preview),
                }

            elif file_format == "json":
                # Get JSON structure info
                with open(file_path, "r", encoding="utf-8") as f:
                    json_data = json.load(f)

                if isinstance(json_data, list):
                    preview_data["auto_detected_config"] = {
                        "structure": "array",
                        "item_count": len(json_data),
                    }
                    sample_data = json_data[:10] if len(json_data) > 10 else json_data
                else:
                    preview_data["auto_detected_config"] = {
                        "structure": "object",
                        "keys": list(json_data.keys()),
                    }
                    sample_data = [json_data]

                preview_data["sample_data"] = sample_data
                preview_data["schema_preview"] = {
                    "structure_type": preview_data["auto_detected_config"]["structure"],
                    "sample_keys": list(sample_data[0].keys()) if sample_data else [],
                }

            return {"success": True, "preview": preview_data}

        except Exception as e:
            logger.error(f"‚ùå File preview generation failed: {str(e)}")
            return {"success": False, "error": str(e)}

    def _initialize_demo_data(self):
        """Initialize demo data sources for testing and demonstration"""
        try:
            logger.info("üöÄ Initializing demo data sources...")

            # Demo sales data
            demo_sales = {
                "id": "demo_sales_data",
                "name": "Demo Sales Data",
                "type": "file",
                "format": "csv",
                "description": "Sample sales data for demonstration and testing",
                "created_at": datetime.now().isoformat(),
                "updated_at": datetime.now().isoformat(),
                "size": 24576,  # 24KB
                "row_count": 1000,
                "schema": {
                    "columns": [
                        {"name": "date", "type": "datetime", "nullable": False},
                        {"name": "product_name", "type": "string", "nullable": False},
                        {"name": "category", "type": "string", "nullable": False},
                        {"name": "sales_amount", "type": "numeric", "nullable": False},
                        {"name": "quantity", "type": "integer", "nullable": False},
                        {"name": "region", "type": "string", "nullable": False},
                        {"name": "customer_id", "type": "string", "nullable": False},
                    ]
                },
                "sample_data": [
                    {
                        "date": "2024-01-15",
                        "product_name": "Laptop Pro",
                        "category": "Electronics",
                        "sales_amount": 1299.99,
                        "quantity": 1,
                        "region": "North",
                        "customer_id": "C001",
                    },
                    {
                        "date": "2024-01-15",
                        "product_name": "Wireless Mouse",
                        "category": "Accessories",
                        "sales_amount": 29.99,
                        "quantity": 2,
                        "region": "South",
                        "customer_id": "C002",
                    },
                    {
                        "date": "2024-01-16",
                        "product_name": "Monitor 4K",
                        "category": "Electronics",
                        "sales_amount": 599.99,
                        "quantity": 1,
                        "region": "East",
                        "customer_id": "C003",
                    },
                    {
                        "date": "2024-01-16",
                        "product_name": "Keyboard",
                        "category": "Accessories",
                        "sales_amount": 89.99,
                        "quantity": 1,
                        "region": "West",
                        "customer_id": "C004",
                    },
                    {
                        "date": "2024-01-17",
                        "product_name": "Tablet",
                        "category": "Electronics",
                        "sales_amount": 399.99,
                        "quantity": 1,
                        "region": "North",
                        "customer_id": "C005",
                    },
                ],
                "metadata": {
                    "source": "demo_data",
                    "business_domain": "retail",
                    "data_quality": "high",
                    "last_updated": datetime.now().isoformat(),
                },
            }

            # Demo customer data
            demo_customers = {
                "id": "demo_customers_data",
                "name": "Demo Customer Data",
                "type": "file",
                "format": "csv",
                "description": "Sample customer data for demonstration and testing",
                "created_at": datetime.now().isoformat(),
                "updated_at": datetime.now().isoformat(),
                "size": 15360,  # 15KB
                "row_count": 500,
                "schema": {
                    "columns": [
                        {"name": "customer_id", "type": "string", "nullable": False},
                        {"name": "first_name", "type": "string", "nullable": False},
                        {"name": "last_name", "type": "string", "nullable": False},
                        {"name": "email", "type": "string", "nullable": False},
                        {"name": "age", "type": "integer", "nullable": True},
                        {"name": "city", "type": "string", "nullable": False},
                        {"name": "country", "type": "string", "nullable": False},
                        {
                            "name": "registration_date",
                            "type": "datetime",
                            "nullable": False,
                        },
                    ]
                },
                "sample_data": [
                    {
                        "customer_id": "C001",
                        "first_name": "John",
                        "last_name": "Doe",
                        "email": "john.doe@email.com",
                        "age": 35,
                        "city": "New York",
                        "country": "USA",
                        "registration_date": "2023-01-15",
                    },
                    {
                        "customer_id": "C002",
                        "first_name": "Jane",
                        "last_name": "Smith",
                        "email": "jane.smith@email.com",
                        "age": 28,
                        "city": "Los Angeles",
                        "country": "USA",
                        "registration_date": "2023-02-20",
                    },
                    {
                        "customer_id": "C003",
                        "first_name": "Bob",
                        "last_name": "Johnson",
                        "email": "bob.johnson@email.com",
                        "age": 42,
                        "city": "Chicago",
                        "country": "USA",
                        "registration_date": "2023-03-10",
                    },
                    {
                        "customer_id": "C004",
                        "first_name": "Alice",
                        "last_name": "Brown",
                        "email": "alice.brown@email.com",
                        "age": 31,
                        "city": "Houston",
                        "country": "USA",
                        "registration_date": "2023-04-05",
                    },
                    {
                        "customer_id": "C005",
                        "first_name": "Charlie",
                        "last_name": "Wilson",
                        "email": "charlie.wilson@email.com",
                        "age": 39,
                        "city": "Phoenix",
                        "country": "USA",
                        "registration_date": "2023-05-12",
                    },
                ],
                "metadata": {
                    "source": "demo_data",
                    "business_domain": "customer_management",
                    "data_quality": "high",
                    "last_updated": datetime.now().isoformat(),
                },
            }

            # Add demo data sources to the registry
            self.data_sources["demo_sales_data"] = demo_sales
            self.data_sources["demo_customers_data"] = demo_customers

            logger.info(
                f"‚úÖ Demo data sources initialized successfully: {list(self.data_sources.keys())}"
            )
            logger.info(f"‚úÖ Total demo sources: {len(self.data_sources)}")

        except Exception as e:
            logger.error(f"‚ùå Failed to initialize demo data: {str(e)}")
            import traceback

            logger.error(f"‚ùå Traceback: {traceback.format_exc()}")

    def _get_current_user_id(self) -> int:
        """Get current user ID from auth context"""
        try:
            # TODO: Implement proper auth context extraction
            # For now, return 1 as default user
            # In production, this should extract user_id from JWT token or request context
            return 1
        except Exception as e:
            logger.warning(f"Failed to get current user ID: {e}, using default")
            return 1

    # üèóÔ∏è PROJECT-SCOPED DATA SOURCE METHODS

    async def get_project_data_sources(
        self,
        organization_id: str,
        project_id: str,
        user_id: str = None,
        offset: int = 0,
        limit: int = 100,
    ) -> List[Dict[str, Any]]:
        """Get data sources for a specific project (project-scoped)"""
        try:
            logger.info(
                f"üîç Getting project data sources for project {project_id} in organization {organization_id}"
            )

            from app.modules.data.models import DataSource
            from app.modules.projects.models import ProjectDataSource
            from app.db.session import get_async_session

            async with get_async_session() as db:
                from sqlalchemy import select

                # Join data sources with project data sources
                query = (
                    select(DataSource)
                    .join(
                        ProjectDataSource,
                        DataSource.id == ProjectDataSource.data_source_id,
                    )
                    .where(
                        # Accept numeric IDs or slugs; if not int, skip casting
                        ProjectDataSource.project_id
                        == (
                            int(project_id)
                            if str(project_id).isdigit()
                            else ProjectDataSource.project_id
                        ),
                        DataSource.is_active,
                        ProjectDataSource.is_active,
                    )
                    .offset(offset)
                    .limit(limit)
                )

                result = await db.execute(query)
                data_sources = result.scalars().all()

                return [
                    {
                        "id": ds.id,
                        "name": ds.name,
                        "type": ds.type,
                        "format": ds.format,
                        "size": ds.size,
                        "row_count": ds.row_count,
                        "schema": ds.schema,
                        "organization_id": organization_id,
                        "project_id": project_id,
                        "created_at": ds.created_at.isoformat()
                        if ds.created_at
                        else None,
                        "updated_at": ds.updated_at.isoformat()
                        if ds.updated_at
                        else None,
                        "last_accessed": ds.last_accessed.isoformat()
                        if ds.last_accessed
                        else None,
                    }
                    for ds in data_sources
                ]
        except Exception as e:
            logger.error(f"‚ùå Failed to get project data sources: {str(e)}")
            return []

    async def create_project_data_source(
        self, organization_id: str, project_id: str, data_source_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Create a new data source for a specific project"""
        try:
            logger.info(
                f"üìä Creating project data source for project {project_id} in organization {organization_id}"
            )

            from app.modules.data.models import DataSource
            from app.modules.projects.models import ProjectDataSource
            from app.db.session import get_async_session

            async with get_async_session() as db:
                # Create the data source
                data_source = DataSource(
                    id=f"ds_{organization_id}_{project_id}_{int(datetime.now().timestamp())}",
                    name=data_source_data["name"],
                    type=data_source_data["type"],
                    format=data_source_data.get("format"),
                    description=data_source_data.get("description"),
                    metadata=json.dumps(data_source_data.get("metadata", {})),
                    user_id=organization_id,  # Using organization_id as user_id for now
                    tenant_id=organization_id,
                    is_active=True,
                    created_at=datetime.now(),
                    updated_at=datetime.now(),
                )

                db.add(data_source)
                await db.flush()  # Get the ID

                # Link to project
                project_data_source = ProjectDataSource(
                    project_id=int(project_id),
                    data_source_id=data_source.id,
                    data_source_type=data_source_data["type"],
                    is_active=True,
                    added_at=datetime.now(),
                )

                db.add(project_data_source)
                await db.commit()

                return {
                    "success": True,
                    "data_source": {
                        "id": data_source.id,
                        "name": data_source.name,
                        "type": data_source.type,
                        "organization_id": organization_id,
                        "project_id": project_id,
                    },
                    "message": "Data source created successfully",
                }
        except Exception as e:
            logger.error(f"‚ùå Failed to create project data source: {str(e)}")
            return {"success": False, "error": str(e)}

    async def get_project_data_source(
        self, organization_id: str, project_id: str, data_source_id: str
    ) -> Dict[str, Any]:
        """Get a specific data source for a project"""
        try:
            logger.info(
                f"üîç Getting project data source {data_source_id} for project {project_id}"
            )

            from app.modules.data.models import DataSource
            from app.modules.projects.models import ProjectDataSource
            from app.db.session import get_async_session

            async with get_async_session() as db:
                from sqlalchemy import select

                query = (
                    select(DataSource)
                    .join(
                        ProjectDataSource,
                        DataSource.id == ProjectDataSource.data_source_id,
                    )
                    .where(
                        DataSource.id == data_source_id,
                        ProjectDataSource.project_id == int(project_id),
                        DataSource.is_active,
                        ProjectDataSource.is_active,
                    )
                )

                result = await db.execute(query)
                data_source = result.scalar_one_or_none()

                if data_source:
                    return {
                        "success": True,
                        "data_source": {
                            "id": data_source.id,
                            "name": data_source.name,
                            "type": data_source.type,
                            "format": data_source.format,
                            "size": data_source.size,
                            "row_count": data_source.row_count,
                            "schema": data_source.schema,
                            "organization_id": organization_id,
                            "project_id": project_id,
                            "created_at": data_source.created_at.isoformat()
                            if data_source.created_at
                            else None,
                            "updated_at": data_source.updated_at.isoformat()
                            if data_source.updated_at
                            else None,
                        },
                    }
                else:
                    return {"success": False, "error": "Data source not found"}
        except Exception as e:
            logger.error(f"‚ùå Failed to get project data source: {str(e)}")
            return {"success": False, "error": str(e)}

    async def execute_project_data_source_query(
        self, organization_id: str, project_id: str, data_source_id: str, query: str
    ) -> Dict[str, Any]:
        """Execute a query on a project data source"""
        try:
            logger.info(f"üîç Executing project data source query for {data_source_id}")

            # First verify the data source belongs to the project
            data_source_result = await self.get_project_data_source(
                organization_id, project_id, data_source_id
            )
            if not data_source_result["success"]:
                return data_source_result

            # Execute the query using the existing method
            return await self.execute_query_on_source(data_source_id, query)
        except Exception as e:
            logger.error(f"‚ùå Failed to execute project data source query: {str(e)}")
            return {"success": False, "error": str(e)}

    async def get_project_data_source_data(
        self,
        organization_id: str,
        project_id: str,
        data_source_id: str,
        limit: int = 100,
    ) -> Dict[str, Any]:
        """Get data from a project data source"""
        try:
            logger.info(f"üìä Getting project data source data for {data_source_id}")

            # First verify the data source belongs to the project
            data_source_result = await self.get_project_data_source(
                organization_id, project_id, data_source_id
            )
            if not data_source_result["success"]:
                return data_source_result

            # Get the data using the existing method
            return await self.get_data_from_source(data_source_id, limit)
        except Exception as e:
            logger.error(f"‚ùå Failed to get project data source data: {str(e)}")
            return {"success": False, "error": str(e)}
